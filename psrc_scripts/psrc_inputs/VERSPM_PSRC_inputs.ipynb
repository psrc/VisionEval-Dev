{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import geopandas as gpd\n",
    "import sqlalchemy\n",
    "from shapely import wkt\n",
    "import h5py\n",
    "import pandana as pdna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_sde(connection_string, feature_class_name, version,\n",
    "                  crs={'init': 'epsg:2285'}, is_table = False):\n",
    "    \"\"\"\n",
    "    Returns the specified feature class as a geodataframe from ElmerGeo.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    connection_string : SQL connection string that is read by geopandas \n",
    "                        read_sql function\n",
    "    \n",
    "    feature_class_name: the name of the featureclass in PSRC's ElmerGeo \n",
    "                        Geodatabase\n",
    "    \n",
    "    cs: cordinate system\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    engine = sqlalchemy.create_engine(connection_string)\n",
    "    con=engine.connect()\n",
    "    #con.execute(\"sde.set_current_version {0}\".format(version))\n",
    "    if is_table:\n",
    "        gdf=pd.read_sql('select * from %s' % \n",
    "                   (feature_class_name), con=con)\n",
    "        con.close()\n",
    "\n",
    "    else:\n",
    "        df=pd.read_sql('select *, Shape.STAsText() as geometry from %s' % \n",
    "                   (feature_class_name), con=con)\n",
    "        con.close()\n",
    "\n",
    "        df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "        gdf=gpd.GeoDataFrame(df, geometry='geometry')\n",
    "        gdf.crs = crs\n",
    "        cols = [col for col in gdf.columns if col not in \n",
    "                ['Shape', 'GDB_GEOMATTR_DATA', 'SDE_STATE_ID']]\n",
    "        gdf = gdf[cols]\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r'C:\\Workspace\\VisionEval\\models\\VERSPM\\inputs_RVMPO'\n",
    "output_dir = r'C:\\Workspace\\VisionEval\\models\\VERSPM\\inputs'\n",
    "\n",
    "regional_geo = 'PSRC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "# Load data that will be used for multiple fields\n",
    "connection_string = 'mssql+pyodbc://AWS-PROD-SQL\\Sockeye/ElmerGeo?driver=SQL Server?Trusted_Connection=yes'\n",
    "crs = {'init' : 'EPSG:2285'}\n",
    "version = \"'DBO.Default'\"\n",
    "gdf_shp = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)\n",
    "gdf_bg = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)\n",
    "\n",
    "run_dir_18 = r'C:\\Workspace\\sc_2018_rtp_final\\soundcast'\n",
    "run_dir_50 = r'L:\\RTP_2022\\final_runs\\sc_rtp_2050_constrained_final\\soundcast'\n",
    "\n",
    "parcel_18 = pd.read_csv(os.path.join(run_dir_18,'inputs\\scenario\\landuse\\parcels_urbansim.txt'), delim_whitespace=True)\n",
    "parcel_50 = pd.read_csv(os.path.join(run_dir_50,'inputs\\scenario\\landuse\\parcels_urbansim.txt'), delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "# Create lookup for parcels to block groups\n",
    "# Load parcel centroids as geodataframe\n",
    "parcel_18_gdf = gpd.GeoDataFrame(\n",
    "    parcel_18, geometry=gpd.points_from_xy(parcel_18.XCOORD_P, parcel_18.YCOORD_P))\n",
    "crs = {'init' : 'EPSG:2285'}\n",
    "parcel_18_gdf.crs = crs\n",
    "\n",
    "parcel_50_gdf = gpd.GeoDataFrame(\n",
    "    parcel_50, geometry=gpd.points_from_xy(parcel_50.XCOORD_P, parcel_50.YCOORD_P))\n",
    "crs = {'init' : 'EPSG:2285'}\n",
    "parcel_50_gdf.crs = crs\n",
    "\n",
    "parcel_18_bg_lookup = gpd.sjoin(gdf_bg, parcel_18_gdf)[['PARCELID','geoid20']]\n",
    "parcel_50_bg_lookup = gpd.sjoin(gdf_bg, parcel_50_gdf)[['PARCELID','geoid20']]\n",
    "\n",
    "parcel_18 = parcel_18.merge(parcel_18_bg_lookup, on='PARCELID')\n",
    "parcel_50 = parcel_50.merge(parcel_50_bg_lookup, on='PARCELID')\n",
    "\n",
    "# # Add the census tract back to the gdf\n",
    "# parcel_18_gdf = parcel_18_gdf.merge(parcel_18_bg_lookup, on='PARCELID')\n",
    "# parcel_50_gdf = parcel_50_gdf.merge(parcel_50_bg_lookup, on='PARCELID')\n",
    "\n",
    "def fill_dummy_parcels(parcel_df, gdf_shp):\n",
    "    # Add some dummy variables for block groups that do not have any parcels \n",
    "    diff_list = list(set(gdf_shp['geoid20'].unique()) - set(parcel_df['geoid20'].unique()))\n",
    "\n",
    "    # Add empty rows that represent this geoid\n",
    "    temp_df = parcel_df.iloc[0:len(diff_list)].copy()\n",
    "\n",
    "    temp_df[temp_df.columns] = 0\n",
    "    temp_df['geoid20'] = diff_list\n",
    "    parcel_df = parcel_df.append(temp_df)\n",
    "\n",
    "    return parcel_df\n",
    "\n",
    "# parcel_18_gdf = fill_dummy_parcels(parcel_18_gdf, gdf_shp)\n",
    "# parcel_50_gdf = fill_dummy_parcels(parcel_50_gdf, gdf_shp)\n",
    "\n",
    "\n",
    "\n",
    "parcel_18 = fill_dummy_parcels(parcel_18, gdf_shp)\n",
    "parcel_50 = fill_dummy_parcels(parcel_50, gdf_shp)\n",
    "\n",
    "# Give dummy parcels an XY coordinate at centroid of their block groups\n",
    "block_group_list = parcel_18[parcel_18['PARCELID'] == 0]['geoid20']\n",
    "\n",
    "parcel_18.loc[parcel_18['PARCELID'] == 0, 'XCOORD_P'] = gdf_shp[gdf_shp['geoid20'].isin(block_group_list)].centroid.x.values\n",
    "parcel_18.loc[parcel_18['PARCELID'] == 0, 'YCOORD_P'] = gdf_shp[gdf_shp['geoid20'].isin(block_group_list)].centroid.y.values\n",
    "# parcel_18[parcel_18['PARCELID'] == 0]\n",
    "\n",
    "\n",
    "# # Give dummy parcels an XY coordinate at centroid of their block groups\n",
    "block_group_list = parcel_50[parcel_50['PARCELID'] == 0]['geoid20']\n",
    "\n",
    "parcel_50.loc[parcel_50['PARCELID'] == 0, 'XCOORD_P'] = gdf_shp[gdf_shp['geoid20'].isin(block_group_list)].centroid.x.values\n",
    "parcel_50.loc[parcel_50['PARCELID'] == 0, 'YCOORD_P'] = gdf_shp[gdf_shp['geoid20'].isin(block_group_list)].centroid.y.values\n",
    "\n",
    "\n",
    "# # Recreate the gdf\n",
    "\n",
    "parcel_18_gdf = gpd.GeoDataFrame(\n",
    "    parcel_18, geometry=gpd.points_from_xy(parcel_18.XCOORD_P, parcel_18.YCOORD_P))\n",
    "crs = {'init' : 'EPSG:2285'}\n",
    "parcel_18_gdf.crs = crs\n",
    "\n",
    "parcel_50_gdf = gpd.GeoDataFrame(\n",
    "    parcel_50, geometry=gpd.points_from_xy(parcel_50.XCOORD_P, parcel_50.YCOORD_P))\n",
    "crs = {'init' : 'EPSG:2285'}\n",
    "parcel_50_gdf.crs = crs\n",
    "\n",
    "# parcel_18_gdf = parcel_18_gdf.merge(parcel_18_bg_lookup, on='PARCELID')\n",
    "# parcel_50_gdf = parcel_50_gdf.merge(parcel_50_bg_lookup, on='PARCELID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcel_18_gdf[parcel_18_gdf['geoid20'] == '530330228033']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Azone</th>\n",
       "      <th>Bzone</th>\n",
       "      <th>Czone</th>\n",
       "      <th>Marea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330001011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330001012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330001013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330001021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330001022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Azone         Bzone  Czone Marea\n",
       "0  PSRC  530330001011    NaN  PSRC\n",
       "1  PSRC  530330001012    NaN  PSRC\n",
       "2  PSRC  530330001013    NaN  PSRC\n",
       "3  PSRC  530330001021    NaN  PSRC\n",
       "4  PSRC  530330001022    NaN  PSRC"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write an initial Geo file\n",
    "# Note that we only want to include zones that have households located in them\n",
    "fname = 'geo.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,'..\\defs',fname))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2928"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define geogrpahy\n",
    "# For now make an exact replica fo the RVMPO data with Azone = region, Bzone = block groups\n",
    "# Ideally we have Azone = county, Bzone = block groups\n",
    "\n",
    "# Get Census tracts from the region\n",
    "# Load  tract geographies from ElmerGeo\n",
    "# NOTE: We are using 2020 geographies;\n",
    "# Use geoid20 as the field name\n",
    "\n",
    "\n",
    "# Load cities shapefile\n",
    "# gdf_cities = read_from_sde(connection_string, 'cities', version, crs=crs, is_table=False)\n",
    "len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_seattle = gdf_cities[gdf_cities['city_name'] == 'Seattle']\n",
    "# geoid_list = gpd.sjoin(gdf_shp, gdf_seattle)['geoid20']\n",
    "\n",
    "# # \n",
    "# df_geog = gdf_shp[['geoid20']]\n",
    "# geoid_list = geoid_list[geoid_list != '530330053041']\n",
    "# ## For now let's just select a sub sample of Block groups in Seattle\n",
    "# df_geog = df_geog[df_geog['geoid20'].isin(geoid_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# geoid_list = ['530330028004','530330028001','530330028002','530330028003']\n",
    "# df_geog = gdf_shp[['geoid20']]\n",
    "# df_geog = df_geog[df_geog['geoid20'].isin(geoid_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pandas\\core\\frame.py:4449: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pandas\\core\\indexing.py:1597: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pandas\\core\\indexing.py:1720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Azone</th>\n",
       "      <th>Bzone</th>\n",
       "      <th>Czone</th>\n",
       "      <th>Marea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330001011</td>\n",
       "      <td>NA</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330001012</td>\n",
       "      <td>NA</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330001013</td>\n",
       "      <td>NA</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330001021</td>\n",
       "      <td>NA</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330001022</td>\n",
       "      <td>NA</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Azone         Bzone Czone Marea\n",
       "0  PSRC  530330001011    NA  PSRC\n",
       "1  PSRC  530330001012    NA  PSRC\n",
       "2  PSRC  530330001013    NA  PSRC\n",
       "3  PSRC  530330001021    NA  PSRC\n",
       "4  PSRC  530330001022    NA  PSRC"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geog = gdf_bg[['geoid20']]\n",
    "df_geog.rename(columns={'geoid20': 'Bzone'}, inplace=True)\n",
    "df_geog.loc[:,'Azone'] = 'PSRC'\n",
    "df_geog.loc[:,'Marea'] = 'PSRC'\n",
    "# df_geog.loc[:,'Czone'] = range(len(df_geog))\n",
    "df_geog.loc[:,'Czone'] = \"NA\"\n",
    "df_geog[df.columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_geog[df.columns].to_csv(os.path.join(output_dir,r'../defs',fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of parcels in the block group area\n",
    "\n",
    "# geoid_list = ['530330028004','530330028001','530330028002','530330028003']\n",
    "# # df_geog = gdf_shp[['geoid20']]\n",
    "# temp_df_geog = gdf_shp[gdf_shp['geoid20'].isin(geoid_list)]\n",
    "# len(df_geog)\n",
    "# parcel_list = gpd.sjoin(temp_df_geog, parcel_18_gdf)['PARCELID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input_dir = r'C:\\Workspace\\VisionEval\\models\\VERSPM\\inputs'\n",
    "# # output_dir = r'C:\\Workspace\\VisionEval\\input_creation\\psrc_inputs'\n",
    "\n",
    "# fname = 'azone_carsvc_characteristics.csv'\n",
    "# df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update A-Zone files\n",
    "Currently reprsenting as for full region\n",
    "According to docs, should be about the size of a PUMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'azone_charging_availability.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many of these we are not yet updating with region-specific numbers and are using the defaults\n",
    "# However, we need to change the region name in the column\n",
    "# additionally, we need to provide specific years of data\n",
    "# Using 2018 and 2050 for years now; we could specify all model years \n",
    "\n",
    "def update_azone(df, name):\n",
    "    df['Geo'] = name\n",
    "    df['Year'] = ['2018','2050']\n",
    "    return df\n",
    "\n",
    "def process_azone(fname):\n",
    "    df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "    df = update_azone(df, 'PSRC')\n",
    "    df.to_csv(os.path.join(output_dir,fname), index=False)\n",
    "\n",
    "\n",
    "# This file specifies the different characteristics for high and low car service level and is used in the \n",
    "# CreateVehicleTable and AssignVehicleAge modules.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_charging_availability.csv'\n",
    "# This file has data on proportion of different household types who has EV charging available and is used in the \n",
    "# AssignHHVehiclePowertrain module.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_electricity_carbon_intensity.csv'\n",
    "# This file is used to specify the carbon intensity of electricity and is optional (only needed if user wants to modify the values). \n",
    "# The file is used in Initialize (VEPowertrainsAndFuels) and CalculateCarbonIntensity modules.\n",
    "process_azone(fname)\n",
    "\n",
    "\n",
    "fname = 'azone_fuel_power_cost.csv'\n",
    "# This file supplies data for retail cost of fuel and electricity and is used in the CalculateVehicleOperatingCost module.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_hh_veh_mean_age.csv' \n",
    "# This file provides inputs for mean auto age and mean light truck age and is used in the AssignVehicleAge module.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_hh_veh_own_taxes.csv' \n",
    "# This file provides inputs for flat fees/taxes (i.e. annual cost per vehicle) and ad valorem taxes (i.e. percentage of vehicle value paid in taxes). The file is used in CalculateVehicleOwnCost module.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_payd_insurance_prop.csv' \n",
    "# This file provides inputs on the proportion of households having PAYD (pay-as-you-drive) insurance and is used in the CalculateVehicleOwnCost module.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_prop_sov_dvmt_diverted.csv' \n",
    "# This file provides inputs for a goal for diverting a portion of SOV travel within a 20-mile tour distance and is used in the DivertSovTravel module.\n",
    "process_azone(fname)\n",
    "\n",
    "# fname = 'azone_relative_employment.csv' \n",
    "# # This file contains ratio of workers to persons by age and is used in the PredictWorkers module.\n",
    "# process_azone(fname)\n",
    "\n",
    "fname = 'azone_veh_use_taxes.csv' \n",
    "# This file supplies data for vehicle related taxes and is used in the CalculateVehicleOperatingCost module.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_vehicle_access_times.csv' \n",
    "# This file supplies data for vehicle acces`s and egress time and is used in the CalculateVehicleOperatingCost module.\n",
    "process_azone(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'azone_gq_pop_by_age.csv' \n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "# This file contains group quarters population estimates/forecasts by age and is used in the CreateHouseholds module.\n",
    "\n",
    "# Get group quarters from parcel file\n",
    "\n",
    "# Taking group quarters populations to be Zero in Soundcast\n",
    "df[['GrpAge0to14','GrpAge15to19','GrpAge20to29','GrpAge30to54','GrpAge55to64','GrpAge65Plus']] = 0 \n",
    "df['Geo'] = regional_geo\n",
    "df['Year'] = ['2018','2050']\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)\n",
    "# parcel_18.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'azone_hh_pop_by_age.csv' \n",
    "# This file contains population estimates/forecasts by age and is used in the CreateHouseholds module.\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "# Get this data from synthetic household/persons\n",
    "\n",
    "\n",
    "def get_age_population(syn_h5, year):\n",
    "\n",
    "    df_person = pd.DataFrame()\n",
    "    for col in syn_h5['Person'].keys():\n",
    "        df_person[col] = syn_h5['Person'][col][:]\n",
    "\n",
    "    # df_hh = pd.DataFrame()\n",
    "    # for col in ['hhno','hhparcel']:\n",
    "    #     df_hh[col] = syn_h5['Household'][col][:]\n",
    "\n",
    "    # df_person = df_person.merge(df_hh, how='left', on='hhno')\n",
    "    # Filter for people living in Seattle only\n",
    "    # df_person = df_person[df_person['hhparcel'].isin(parcel_list)]\n",
    "    col_list = [i for i in df.columns if i not in ['Geo','Year']]\n",
    "\n",
    "    # Separate ages into groups\n",
    "    bins = [-1,14,19,29,54,64,200]\n",
    "    df_person['new_group'] = pd.cut(df_person['pagey'], bins, labels=col_list)\n",
    "\n",
    "    _df = df_person.groupby('new_group').count()[['psexpfac']].reset_index().T\n",
    "    _df = _df.reset_index(drop=True)\n",
    "    _df.columns = _df.iloc[0]\n",
    "    _df = _df.iloc[1:]\n",
    "    # _df.drop('new_group', axis=1, inplace=True)\n",
    "    _df['Year'] = year\n",
    "    _df['Geo'] = 'PSRC'\n",
    "    _df = _df.reset_index(drop=True)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "syn_h5 = h5py.File(r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2018\\new_emp\\hh_and_persons.h5', 'r')\n",
    "_df_18 = get_age_population(syn_h5, '2018')\n",
    "_df_18 = _df_18[df.columns]\n",
    "syn_h5.close()\n",
    "\n",
    "syn_h5 = h5py.File(r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2050\\rtp_2050\\hh_and_persons.h5', 'r')\n",
    "_df_50 = get_age_population(syn_h5, '2050')\n",
    "_df_50 = _df_50[df.columns]\n",
    "syn_h5.close()\n",
    "\n",
    "_df = _df_18.append(_df_50)\n",
    "_df.to_csv(os.path.join(output_dir,fname), index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'azone_hhsize_targets.csv' \n",
    "# This file contains the household specific targets and is used in CreateHouseholds module.\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "\n",
    "def get_hhsize_targets(syn_h5):\n",
    "    df_hh = pd.DataFrame()\n",
    "    for col in syn_h5['Household'].keys():\n",
    "        df_hh[col] = syn_h5['Household'][col][:]\n",
    "    mean_hh_size = df_hh['hhsize'].mean()\n",
    "    one_person_hh = len(df_hh[df_hh['hhsize'] == 1])/len(df_hh)\n",
    "    \n",
    "    return mean_hh_size, one_person_hh\n",
    "\n",
    "df['Year'] = ['2018','2050']\n",
    "df['Geo'] = 'PSRC'\n",
    "\n",
    "syn_h5 = h5py.File(r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2018\\new_emp\\hh_and_persons.h5', 'r')\n",
    "mean_hh_size, one_person_hh = get_hhsize_targets(syn_h5)\n",
    "df.loc[df['Year'] == '2018', 'AveHhSize'] =  mean_hh_size\n",
    "df.loc[df['Year'] == '2018', 'Prop1PerHh'] =  one_person_hh\n",
    "\n",
    "syn_h5 = h5py.File(r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2050\\rtp_2050\\hh_and_persons.h5', 'r')\n",
    "mean_hh_size, one_person_hh = get_hhsize_targets(syn_h5)\n",
    "df.loc[df['Year'] == '2050', 'AveHhSize'] =  mean_hh_size\n",
    "df.loc[df['Year'] == '2050', 'Prop1PerHh'] =  one_person_hh\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_person = pd.DataFrame()\n",
    "syn_h5 = h5py.File(r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2018\\new_emp\\hh_and_persons.h5', 'r')\n",
    "for col in syn_h5['Person'].keys():\n",
    "    df_person[col] = syn_h5['Person'][col][:]\n",
    "# mean_hh_size = df_hh['hhsize'].mean()\n",
    "# one_person_hh = len(df_hh[df_hh['hhsize'] == 1])/len(df_hh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42405.91717289795"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hh['hhincome'].sum()/len(df_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'azone_lttrk_prop.csv' \n",
    "# This file specifies the light truck proportion of the vehicle fleet and is used in AssignVehicleType module.\n",
    "# This refers I believe to the main vehicle population; \n",
    "# Not changing the default for now, but should be looked up via MOVES or assumed flat?\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Geo'] = 'PSRC'\n",
    "df['Year'] = ['2018','2050']\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'azone_per_cap_inc.csv' \n",
    "# This file contains information on regional average per capita household and group quarters income in year 2010 dollars and is used in the PredictIncome module.\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Geo'] = 'PSRC'\n",
    "df['Year'] = ['2018','2050']\n",
    "\n",
    "# Get average income from h5 files\n",
    "syn_h5 = h5py.File(r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2018\\new_emp\\hh_and_persons.h5', 'r')\n",
    "df_hh = pd.DataFrame()\n",
    "for col in syn_h5['Household'].keys():\n",
    "    df_hh[col] = syn_h5['Household'][col][:]\n",
    "# df_hh = df_hh[df_hh['hhparcel'].isin(parcel_list)]    \n",
    "df_hh['per_cap_income'] = df_hh['hhincome']/df_hh['hhsize']\n",
    "df.loc[df['Year'] == '2018', ['HHIncomePC.2010','GQIncomePC.2010']] = df_hh['per_cap_income'].mean()\n",
    "\n",
    "syn_h5 = h5py.File(r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2050\\rtp_2050\\hh_and_persons.h5', 'r')\n",
    "df_hh = pd.DataFrame()\n",
    "for col in syn_h5['Household'].keys():\n",
    "    df_hh[col] = syn_h5['Household'][col][:]\n",
    "# df_hh = df_hh[df_hh['hhparcel'].isin(parcel_list)] \n",
    "df_hh['per_cap_income'] = df_hh['hhincome']/df_hh['hhsize']\n",
    "df.loc[df['Year'] == '2050', ['HHIncomePC.2010','GQIncomePC.2010']] = df_hh['per_cap_income'].mean()\n",
    "\n",
    "df.rename(columns={'HHIncomePC.2010': 'HHIncomePC.2018', 'GQIncomePC.2010': 'GQIncomePC.2018'}, inplace=True)\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# Get average vehicles per driver\n",
    "#### FIXME: \n",
    "### us this to update the file directly\n",
    "df = pd.read_csv(r'L:\\RTP_2022\\final_runs\\sc_2018_rtp_final\\soundcast\\outputs\\daysim\\_household.tsv', delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3069100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hhvehs'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3314941"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get total number of adults\n",
    "df['hhsize'].sum()-df['hhcu5'].sum()-df['hh515'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925838499086409"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3069100/3314941.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B Zones\n",
    "Block Group Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'bzone_transit_service.csv' \n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "# D4c\n",
    "# Aggregate frequency of transit service within 0.25 miles of block group boundary per hour during evening peak period \n",
    "# (Ref: EPA 2010 Smart Location Database) from GTFS.\n",
    "# See pg 23 https://www.epa.gov/sites/default/files/2021-06/documents/epa_sld_3.0_technicaldocumentationuserguide_may2021.pdf\n",
    "# Aggregate Frequency of Peak Hour Transit Service (D4c) \n",
    "\n",
    "# FIXME: do a comparison to our calculation to make sure it lines up with the EPA data for base year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transit_service(run_dir, year):\n",
    "    # Transit stops do not have frequencies so we need to load the transit lines\n",
    "    df_transit = gpd.read_file(os.path.join(run_dir,r'inputs\\scenario\\networks\\shapefiles\\AM\\AM_edges.shp'))\n",
    "\n",
    "    df = pd.read_csv(os.path.join(run_dir,r'inputs\\scenario\\networks\\shapefiles\\AM\\AM_transit_segments.csv'))\n",
    "    df_headways = pd.read_csv(os.path.join(run_dir,r'inputs\\scenario\\networks\\headways.csv'))\n",
    "    df = df.merge(df_headways[['LineID','hdw_16to17']])\n",
    "\n",
    "    df_transit = df_transit.merge(df[['LineID','hdw_16to17','ij']], left_on='id', right_on='ij')\n",
    "\n",
    "    gdf_bg = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)\n",
    "    # Apply a quarter mile buffer around each block group\n",
    "    gdf_bg.geography = gdf_bg.buffer(5280/4.0)\n",
    "\n",
    "    gdf_joined = gpd.sjoin(gdf_bg, df_transit, how='left')\n",
    "\n",
    "    bg_headways = gdf_joined.groupby(['geoid20','LineID']).mean()[['hdw_16to17']].reset_index()\n",
    "\n",
    "    bg_headways['D4c'] = 60.0/bg_headways['hdw_16to17']\n",
    "    bg_headways['D4c'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "    bg_headways = bg_headways.groupby('geoid20').sum()[['D4c']]\n",
    "    bg_headways = bg_headways.reset_index()\n",
    "\n",
    "    # Make sure to include all block groups even if they have no transit\n",
    "    add_df = gdf_bg[-gdf_bg['geoid20'].isin(bg_headways['geoid20'])][['geoid20']]\n",
    "    add_df['D4c'] = 0\n",
    "    bg_headways = bg_headways.append(add_df)\n",
    "\n",
    "    bg_headways.rename(columns={'geoid20': 'Geo'}, inplace=True)\n",
    "    bg_headways['Year'] = year\n",
    "\n",
    "    # bg_headways = bg_headways[bg_headways['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "    geo_df = pd.read_csv(os.path.join(output_dir,r'..\\defs\\geo.csv'))\n",
    "    bg_headways = bg_headways[bg_headways['Geo'].isin(geo_df['Bzone'].astype('str'))]\n",
    "\n",
    "    return bg_headways\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\geopandas\\geodataframe.py:182: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  super(GeoDataFrame, self).__setattr__(attr, val)\n"
     ]
    }
   ],
   "source": [
    "run_dir = r'C:\\Workspace\\sc_2018_rtp_final\\soundcast'\n",
    "df = calculate_transit_service(run_dir, '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\geopandas\\geodataframe.py:182: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  super(GeoDataFrame, self).__setattr__(attr, val)\n"
     ]
    }
   ],
   "source": [
    "run_dir = r'L:\\RTP_2022\\final_runs\\sc_rtp_2050_constrained_final\\soundcast'\n",
    "df_50 = calculate_transit_service(run_dir, '2050')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df_50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geo</th>\n",
       "      <th>D4c</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530330001011</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>530330001012</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>530330001013</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>530330001021</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>530330001022</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>530610538032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>530610538034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>530619400022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>530619400023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>530619901000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5856 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Geo   D4c  Year\n",
       "0     530330001011  11.0  2018\n",
       "1     530330001012  23.0  2018\n",
       "2     530330001013  23.0  2018\n",
       "3     530330001021  12.0  2018\n",
       "4     530330001022  35.0  2018\n",
       "...            ...   ...   ...\n",
       "2915  530610538032   0.0  2050\n",
       "2917  530610538034   0.0  2050\n",
       "2924  530619400022   0.0  2050\n",
       "2925  530619400023   0.0  2050\n",
       "2927  530619901000   0.0  2050\n",
       "\n",
       "[5856 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_bg[gdf_bg['geoid20'] == '530330017024']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Process data at Block Group Level\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'bzone_carsvc_availability.csv'\n",
    "# This file contains the information about level of car service availability and is used in the AssignCarSvcAvailability module.\n",
    "\n",
    "#######################\n",
    "# FIXME: update with values based on density or some other data\n",
    "########################\n",
    "\n",
    "# Extract something about average weight times or population from existing model. \n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "_gdf_bg = gdf_bg[['geoid20']].copy()\n",
    "_gdf_bg.rename(columns={'geoid20': 'Geo'}, inplace=True)\n",
    "_gdf_bg_18 = _gdf_bg.copy()\n",
    "_gdf_bg_18['Year'] = '2018'\n",
    "_gdf_bg_18['CarSvcLevel'] = 'High'\n",
    "\n",
    "_gdf_bg_50 = _gdf_bg_18.copy()\n",
    "_gdf_bg_50['Year'] = '2050'\n",
    "\n",
    "df = _gdf_bg_18.append(_gdf_bg_50)\n",
    "\n",
    "# geo_df = pd.read_csv(os.path.join(output_dir,r'..\\defs\\geo.csv'))\n",
    "# df = df[df['Geo'].isin(geo_df['Bzone'])]\n",
    "\n",
    "# df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['geoid20'] =]\n",
    "\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'bzone_dwelling_units.csv' \n",
    "# This file contains the number single-family, multi-family and group-quarter dwelling units and is used in the PredictHousing module.\n",
    "\n",
    "# Extract from parcel file; assuming 0 group quarter units \n",
    "df = parcel_18.groupby('geoid20').sum()[['SFUNITS','MFUNITS']].reset_index()\n",
    "df.rename(columns={'SFUNITS': 'SFDU', 'MFUNITS': 'MFDU', 'geoid20': 'Geo'}, inplace=True)\n",
    "df['GQDU'] = 0\n",
    "# df.iloc[0:6]['GQDU'] = 100     ################# FIXME: temp test\n",
    "\n",
    "# Make sure we have values for all block groups\n",
    "df['Year'] = '2018'\n",
    "\n",
    "df_50 = parcel_18.groupby('geoid20').sum()[['SFUNITS','MFUNITS']].reset_index()\n",
    "df_50.rename(columns={'SFUNITS': 'SFDU', 'MFUNITS': 'MFDU', 'geoid20': 'Geo'}, inplace=True)\n",
    "df_50['GQDU'] = 0\n",
    "# df.iloc[0:6]['GQDU'] = 100     ################# FIXME: temp test\n",
    "df_50['Year'] = '2050'\n",
    "# SFDU, fmdu, GQDU\n",
    "\n",
    "df = df.append(df_50)\n",
    "\n",
    "# df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geo</th>\n",
       "      <th>SFDU</th>\n",
       "      <th>MFDU</th>\n",
       "      <th>GQDU</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>530330004043</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>530330047022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>530330053031</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>530330053032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>530330053041</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td>530530729091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2356</th>\n",
       "      <td>530610401004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2369</th>\n",
       "      <td>530610407001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>530610417043</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>530619901000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Geo  SFDU  MFDU  GQDU  Year\n",
       "22    530330004043     0     0     0  2018\n",
       "176   530330047022     0     0     0  2018\n",
       "202   530330053031     0     0     0  2018\n",
       "203   530330053032     0     0     0  2018\n",
       "204   530330053041     0     0     0  2018\n",
       "...            ...   ...   ...   ...   ...\n",
       "2203  530530729091     0     0     0  2050\n",
       "2356  530610401004     0     0     0  2050\n",
       "2369  530610407001     0     0     0  2050\n",
       "2441  530610417043     0     0     0  2050\n",
       "2927  530619901000     0     0     0  2050\n",
       "\n",
       "[118 rows x 5 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[['SFDU','MFDU']].sum(axis=1) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'bzone_employment.csv' \n",
    "# This file contains the total, retail and service employment by zone and is used in the LocateEmployment module.\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "# 'Geo', 'Year', 'TotEmp', 'RetEmp', 'SvcEmp'\n",
    "df = parcel_18.groupby('geoid20').sum()[['EMPRET_P','EMPSVC_P','EMPTOT_P']].reset_index()\n",
    "df.rename(columns={'EMPRET_P': 'RetEmp', 'EMPSVC_P': 'SvcEmp', 'EMPTOT_P': 'TotEmp', 'geoid20': 'Geo'}, inplace=True)\n",
    "df['Year'] = '2018'\n",
    "\n",
    "df_50 = parcel_50.groupby('geoid20').sum()[['EMPRET_P','EMPSVC_P','EMPTOT_P']].reset_index()\n",
    "df_50.rename(columns={'EMPRET_P': 'RetEmp', 'EMPSVC_P': 'SvcEmp', 'EMPTOT_P': 'TotEmp', 'geoid20': 'Geo'}, inplace=True)\n",
    "df_50['Year'] = '2050'\n",
    "\n",
    "df = df.append(df_50)\n",
    "\n",
    "# df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "fname = 'bzone_hh_inc_qrtl_prop.csv'\n",
    "#  This file contains the proportion of households in 1st, 2nd, 3rd, and 4th quartile of household income and is used in the PredictHousing module.\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "def avg_income(year, run_dir):\n",
    "    # Get average income from h5 files\n",
    "    syn_h5 = h5py.File(os.path.join(run_dir, r'hh_and_persons.h5'), 'r')\n",
    "    df_hh = pd.DataFrame()\n",
    "    for col in syn_h5['Household'].keys():\n",
    "        df_hh[col] = syn_h5['Household'][col][:]\n",
    "    # df.loc[df['Year'] == '2018', ['HHIncomePC.2010','GQIncomePC.2010']] = df_hh['hhincome'].mean()\n",
    "\n",
    "\n",
    "    # Join the block group to the household\n",
    "    df_hh = df_hh.merge(parcel_18[['PARCELID','geoid20']], left_on='hhparcel', right_on='PARCELID')\n",
    "\n",
    "    _df = pd.DataFrame(pd.qcut(df_hh['hhincome'], 4, labels=['1','2','3','4']))\n",
    "    df_hh = df_hh.merge(_df, left_index=True, right_index=True)\n",
    "    _df = df_hh.pivot_table(index='geoid20', columns='hhincome_y', values='hhexpfac', aggfunc='sum')\n",
    "\n",
    "    df_sum = pd.DataFrame(_df[['1','2','3','4']].sum(axis=1)).reset_index()\n",
    "    df_sum.rename(columns={0: 'total_hh', 'geoid20': 'Geo'}, inplace=True)\n",
    "    _df = df_sum.merge(_df, left_on='Geo', right_index=True)\n",
    "    for i in [1,2,3,4]:\n",
    "        _df['HhPropIncQ'+str(i)] = _df[str(i)]/_df['total_hh']\n",
    "\n",
    "    # Make sure all block groups are available\n",
    "    full_gdf_bg = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)\n",
    "    missing_bg = full_gdf_bg[~full_gdf_bg['geoid20'].isin(_df['Geo'])][['geoid20']]\n",
    "    # print(missing_bg)\n",
    "    temp_df = _df.iloc[:len(missing_bg)].copy()\n",
    "    # Set income distributions for empty block groups to uniform distribution\n",
    "    temp_df[['HhPropIncQ1','HhPropIncQ2','HhPropIncQ3', 'HhPropIncQ4']] = 0.25\n",
    "    temp_df['Geo'] = missing_bg['geoid20'].values\n",
    "    _df = _df.append(temp_df)\n",
    "\n",
    "    # Filter for rows in the geo file\n",
    "    #### FIXME: This should depend on the final list of zones to be included\n",
    "    geo_df = pd.read_csv(os.path.join(output_dir,r'..\\defs\\geo.csv'))\n",
    "    _df = _df[_df['Geo'].isin(geo_df['Bzone'].astype('str'))]\n",
    "    \n",
    "    # _df = _df[_df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "    _df['Year'] = year\n",
    "\n",
    "    return _df[df.columns]\n",
    "\n",
    "df = avg_income('2018', r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2018\\new_emp')\n",
    "df_50 = avg_income('2050', r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2050\\lodes')\n",
    "df = df.append(df_50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geo</th>\n",
       "      <th>Year</th>\n",
       "      <th>HhPropIncQ1</th>\n",
       "      <th>HhPropIncQ2</th>\n",
       "      <th>HhPropIncQ3</th>\n",
       "      <th>HhPropIncQ4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530330053041</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530330053041</td>\n",
       "      <td>2050</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Geo  Year  HhPropIncQ1  HhPropIncQ2  HhPropIncQ3  HhPropIncQ4\n",
       "0  530330053041  2018         0.25         0.25         0.25         0.25\n",
       "0  530330053041  2050         0.25         0.25         0.25         0.25"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Geo'] == '530330053041']\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_dir = r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2018\\new_emp'\n",
    "# syn_h5 = h5py.File(os.path.join(run_dir, r'hh_and_persons.h5'), 'r')\n",
    "# df_hh = pd.DataFrame()\n",
    "# for col in syn_h5['Household'].keys():\n",
    "#     df_hh[col] = syn_h5['Household'][col][:]\n",
    "# # df.loc[df['Year'] == '2018', ['HHIncomePC.2010','GQIncomePC.2010']] = df_hh['hhincome'].mean()\n",
    "\n",
    "\n",
    "# # Join the block group to the household\n",
    "# df_hh = df_hh.merge(parcel_18[['PARCELID','geoid20']], left_on='hhparcel', right_on='PARCELID')\n",
    "\n",
    "# _df = pd.DataFrame(pd.qcut(df_hh['hhincome'], 4, labels=['1','2','3','4']))\n",
    "# df_hh = df_hh.merge(_df, left_index=True, right_index=True)\n",
    "# _df = df_hh.pivot_table(index='geoid20', columns='hhincome_y', values='hhexpfac', aggfunc='sum')\n",
    "\n",
    "# df_sum = pd.DataFrame(_df[['1','2','3','4']].sum(axis=1)).reset_index()\n",
    "# df_sum.rename(columns={0: 'total_hh', 'geoid20': 'Geo'}, inplace=True)\n",
    "# _df = df_sum.merge(_df, left_on='Geo', right_index=True)\n",
    "# for i in [1,2,3,4]:\n",
    "#     _df['HhPropIncQ'+str(i)] = _df[str(i)]/_df['total_hh']\n",
    "\n",
    "# # Make sure all block groups are available\n",
    "# # full_gdf_bg = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)\n",
    "# # missing_bg = full_gdf_bg[~full_gdf_bg['geoid20'].isin(_df['Geo'])]['geoid20']\n",
    "# # print(missing_bg)\n",
    "# # temp_df = _df.iloc[:len(missing_bg)].copy()\n",
    "# # # Set income distributions for empty block groups to uniform distribution\n",
    "# # temp_df[['HhPropIncQ1','HhPropIncQ2','HhPropIncQ3', 'HhPropIncQ4']] = 0.25\n",
    "# # temp_df['Geo'] = missing_bg['geoid20'].values\n",
    "# # _df = _df.append(temp_df)\n",
    "\n",
    "# syn_h5.close()\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows in the geo file\n",
    "#### FIXME: This should depend on the final list of zones to be included\n",
    "# geo_df = pd.read_csv(os.path.join(output_dir,r'..\\defs\\geo.csv'))\n",
    "# _df = _df[_df['Geo'].isin(geo_df['Bzone']geo_df['Bzone'].astype('str'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df[_df['Geo'].isin(geo_df['Bzone'].astype('str'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'bzone_lat_lon.csv' \n",
    "# This file contains the latitude and longitude of the centroid of the zone and is used in the LocateEmployment module.\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "# Get from intptlat and intpltlon\n",
    "###########################\n",
    "#### FIXME: ensure these are the correct values, not sure how they are calculated or their source\n",
    "###########################\n",
    "gdf_bg_18 = gdf_bg.copy()\n",
    "gdf_bg_18.rename(columns={'intptlat': 'Latitude', 'intptlon': 'Longitude', 'geoid20': 'Geo'}, inplace=True)\n",
    "gdf_bg_18['Year'] = '2018'\n",
    "\n",
    "gdf_bg_50 = gdf_bg_18.copy()\n",
    "gdf_bg_50['Year'] = '2050'\n",
    "\n",
    "df = gdf_bg_18.append(gdf_bg_50)[['Geo', 'Year', 'Latitude', 'Longitude']]\n",
    "\n",
    "# df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'bzone_network_design.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "#  This file contains the intersection density in terms of pedestrian-oriented intersections having four or more legs per square mile and is used in the Calculate4DMeasures module.\n",
    "\n",
    "# d3bpo4 https://www.epa.gov/sites/default/files/2021-06/documents/epa_sld_3.0_technicaldocumentationuserguide_may2021.pdf\n",
    "# Intersection density in terms of pedestrian-oriented\n",
    "# intersections having four or more legs per square mile \n",
    "\n",
    "\n",
    "#######\n",
    "## Let's just use pandana lib for this\n",
    "def calculate_intersections(run_dir, year):\n",
    "    nodes = pd.read_csv(os.path.join(run_dir, r'inputs\\base_year\\all_streets_nodes.csv'), \n",
    "                    index_col='node_id')\n",
    "    links = pd.read_csv(os.path.join(run_dir, r'inputs\\base_year\\all_streets_links.csv'), \n",
    "                    index_col=None)\n",
    "\n",
    "    # get rid of circular links\n",
    "    links = links.loc[(links.from_node_id != links.to_node_id)]\n",
    "\n",
    "    # assign impedance\n",
    "    imp = pd.DataFrame(links.Shape_Length)\n",
    "    imp = imp.rename(columns = {'Shape_Length':'distance'})\n",
    "    links[['from_node_id','to_node_id']] = links[['from_node_id','to_node_id']].astype('int') \n",
    "\n",
    "    net = pdna.network.Network(nodes.x, nodes.y, links.from_node_id, links.to_node_id, imp)\n",
    "    all_nodes = pd.DataFrame(net.edges_df['from'].append(net.edges_df.to), columns = ['node_id'])\n",
    "\n",
    "    # get the frequency of each node, which is the number of intersecting ways\n",
    "    intersections_df = pd.DataFrame(all_nodes.node_id.value_counts())\n",
    "    intersections_df = intersections_df.rename(columns = {'node_id' : 'edge_count'})\n",
    "    intersections_df.reset_index(0, inplace = True)\n",
    "    intersections_df = intersections_df.rename(columns = {'index' : 'node_ids'})\n",
    "\n",
    "    # add a column for each way count\n",
    "    intersections_df['nodes4'] = np.where(intersections_df['edge_count']>3, 1, 0)\n",
    "\n",
    "    df = nodes.merge(intersections_df, left_index=True, right_on='node_ids')\n",
    "    # filter for all 4-way intersections\n",
    "    df = df[df['nodes4'] > 0]\n",
    "\n",
    "    # Convert to geopandas dataframe\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, geometry=gpd.points_from_xy(df.x, df.y))\n",
    "    crs = {'init' : 'EPSG:2285'}\n",
    "    gdf.crs = crs\n",
    "\n",
    "    gdf = gpd.sjoin(gdf, gdf_bg)\n",
    "\n",
    "    df = gdf.groupby('geoid20').sum()[['nodes4']]\n",
    "\n",
    "    # Join with other block groups not including any 4-way intersections\n",
    "    df = gdf_bg.merge(df, on='geoid20', how='left')[['geoid20', 'nodes4']]\n",
    "    df['nodes4'].fillna(0, inplace=True)\n",
    "    df.rename(columns={'nodes4': 'D3bpo4', 'geoid20': 'Geo'}, inplace=True)\n",
    "    df['Year'] = year\n",
    "\n",
    "    # #### FIXME: This should depend on the final list of zones to be included\n",
    "    # geo_df = pd.read_csv(os.path.join(output_dir,r'..\\defs\\geo.csv'))\n",
    "    # df = df[df['Geo'].isin(geo_df['Bzone'])]\n",
    "\n",
    "    # df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "df_18 = calculate_intersections(run_dir_18, '2018')\n",
    "df_50 = calculate_intersections(run_dir_50, '2050')\n",
    "df = df_18.append(df_50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['Geo'] == '530330012022']\n",
    "# # gdf_bg[gdf_bg['geoid20'] == '530330012022']\n",
    "# [print(i) for i in df['Geo'].unique()]\n",
    "# df[df['Geo'] == 'NA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'bzone_parking.csv' \n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df\n",
    "# This file contains the parking information and is used in the AssignParkingRestrictions module.\n",
    "\n",
    "# PkgSpacesPerSFDU: Average number of free parking spaces available to residents of single-family dwelling units\n",
    "# PkgSpacesPerMFDU: Average number of free parking spaces available to residents of multifamily dwelling units\n",
    "# PkgSpacesPerGQ: Average number of free parking spaces available to group quarters residents\n",
    "# PropWkrPay: Proportion of workers who pay for parking\n",
    "# PropCashOut: Proportions of workers paying for parking in a cash-out-buy-back program\n",
    "# PkgCost: Average daily cost for long-term parking (e.g. paid on monthly basis)\n",
    "\n",
    "### FIXME: see if we can get this info; shouldn't come from parcel data becasue this is limited to on-street parking\n",
    "\n",
    "# df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "# df\n",
    "# should mostly be available from parcels file\n",
    "\n",
    "# _df['avg_dy_price'] = _df['PARKDY_P']/_df['PPRICDYP']\n",
    "# _df['avg_hr_price'] = _df['PARKHR_P']/_df['PPRICHRP']\n",
    "\n",
    "def get_parking_restrictions(year):\n",
    "##### FIXME\n",
    "#############\n",
    "    _df = parcel_18.groupby('geoid20').sum()[['PARKDY_P','PARKHR_P','SFUNITS','MFUNITS']]\n",
    "    _df = _df.reset_index()\n",
    "    _df['PkgSpacesPerSFDU'] = 2    # Set equal to 2 for now\n",
    "    _df['PkgSpacesPerMFDU'] = 0.5    # Set equal to 0.5 for now\n",
    "    _df['PkgSpacesPerGQ'] = 0.5     # set to same as MF ?\n",
    "\n",
    "    df.columns\n",
    "    # \n",
    "    # PropWrkPay\n",
    "    # Is a variable in daysim but we do not populate it\n",
    "    _df_mean = parcel_18.groupby('geoid20').mean()[['PPRICDYP','PPRICHRP']]\n",
    "    _df_mean = _df_mean.reset_index()\n",
    "    _df = _df.merge(_df_mean, on='geoid20', how='left')\n",
    "    _df['paid'] = 0\n",
    "    _df.loc[_df['PPRICHRP'] > 0, 'paid'] = 1\n",
    "    _df['PropNonWrkTripPay'] = _df['paid']\n",
    "    _df['PropWkrPay'] = _df['paid']\n",
    "    _df['PropCashOut'] = 0\n",
    "    _df['PkgCost.2010'] = _df['PPRICDYP']/100\n",
    "\n",
    "    _df.rename(columns={'geoid20': 'Geo', }, inplace=True)\n",
    "    _df['Year'] = year\n",
    "\n",
    "    return _df\n",
    "\n",
    "df18 = get_parking_restrictions('2018')[df.columns]\n",
    "df50 = get_parking_restrictions('2050')[df.columns]\n",
    "df = df18.append(df50)\n",
    "\n",
    "# dfparcel = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "# df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['PropNonWrkTripPay']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df[_df['PARKHR_P'] >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'bzone_travel_demand_mgt.csv'\n",
    "_df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "# This file contains the information about workers and households participating in demand management programs and is used in the AssignDemandManagement module.\n",
    "\n",
    "def get_tdm(year):\n",
    "    _df = parcel_18.groupby('geoid20').count()[['PARCELID']].reset_index()\n",
    "    _df = _df.reset_index()\n",
    "    # could use output of transit pass ownership model or apply some off model thing based on survey data\n",
    "    # FIXME: update this from some other attribute\n",
    "\n",
    "    # EcoProp: Proportion of workers working in Bzone who participate in strong employee commute options program\n",
    "    # ImpProp: Proportion of households residing in Bzone who participate in strong individualized marketing program\n",
    "    _df['EcoProp'] = 0\n",
    "    _df['ImpProp'] = 0\n",
    "    _df.rename(columns={'geoid20': 'Geo'}, inplace=True)\n",
    "    _df['Year'] = year\n",
    "\n",
    "    return _df\n",
    "\n",
    "df18 = get_tdm('2018')[_df.columns]\n",
    "df50 = get_tdm('2050')[_df.columns]\n",
    "df = df18.append(df50)\n",
    "\n",
    "# df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geo</th>\n",
       "      <th>Year</th>\n",
       "      <th>EcoProp</th>\n",
       "      <th>ImpProp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530330001011</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>530330001012</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>530330001013</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>530330001021</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>530330001022</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923</th>\n",
       "      <td>530619400021</td>\n",
       "      <td>2050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>530619400022</td>\n",
       "      <td>2050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>530619400023</td>\n",
       "      <td>2050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926</th>\n",
       "      <td>530619900020</td>\n",
       "      <td>2050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>530619901000</td>\n",
       "      <td>2050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5856 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Geo  Year  EcoProp  ImpProp\n",
       "0     530330001011  2018        0        0\n",
       "1     530330001012  2018        0        0\n",
       "2     530330001013  2018        0        0\n",
       "3     530330001021  2018        0        0\n",
       "4     530330001022  2018        0        0\n",
       "...            ...   ...      ...      ...\n",
       "2923  530619400021  2050        0        0\n",
       "2924  530619400022  2050        0        0\n",
       "2925  530619400023  2050        0        0\n",
       "2926  530619900020  2050        0        0\n",
       "2927  530619901000  2050        0        0\n",
       "\n",
       "[5856 rows x 4 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fname = 'bzone_unprotected_area.csv'\n",
    "# This file contains the information about unprotected (i.e., developable) area within the zone and is used in the Calculate4DMeasures module.\n",
    "_df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "# get developable area from UGA boundaries\n",
    "# ElmerGeo.DBO.tod_prcl_uga\n",
    "# gdf_shp = read_from_sde(connection_string, 'tod_prcl_uga', version, crs=crs, is_table=False)\n",
    "# gdf_shp = read_from_sde(connection_string, 'urban_growth_area', version, crs=crs, is_table=False)\n",
    "# gdf_shp = gdf_shp.dissolve()\n",
    "# gdf_shp['in_uga'] = 1\n",
    "\n",
    "def get_unprotected_area(year, _df):\n",
    "    # Use the block group shapefile with no water because we are calculating buildable area\n",
    "    gdf_bg = read_from_sde(connection_string, 'blockgrp2020_nowater', version, crs=crs, is_table=False)\n",
    "    gdf_bg['total_area'] = gdf_bg.area\n",
    "\n",
    "    # Add rural/town/urban definitions using regional geographies\n",
    "    rg_shp = read_from_sde(connection_string, 'regional_geographies_preferred_alternative', version, crs=crs, is_table=False)\n",
    "\n",
    "    # Load the UGA bounds\n",
    "    # Create these defintions based on regional geographies and UGA?\n",
    "    # UrbanArea: Area that is Urban and unprotected (i.e. developable) within the zone (Acres)\n",
    "    # TownArea: Area that is Town and unprotected within the zone (Acres)\n",
    "    # RuralArea: Area that is Rural and unprotected within the zone (Acres)\n",
    "    rg_shp['rg_propose_pa'].unique()\n",
    "    # rg_shp.columns\n",
    "    rg_dict = {'CitiesTowns': 'TownArea',\n",
    "                'Core': 'UrbanArea',\n",
    "                'UU': 'UrbanArea',\n",
    "                'Metro': 'UrbanArea',\n",
    "                'HCT': 'UrbanArea',\n",
    "                'Rural': 'RuralArea'}\n",
    "    rg_shp['urban_rural'] = rg_shp['rg_propose_pa'].map(rg_dict)\n",
    "\n",
    "    join_rg_gdf = gpd.overlay(gdf_bg, rg_shp, how='intersection')\n",
    "\n",
    "    join_rg_gdf['rg_area'] = join_rg_gdf.area\n",
    "\n",
    "    df = join_rg_gdf.pivot_table(index='geoid20', columns='urban_rural', aggfunc='sum', values='rg_area').fillna(0)\n",
    "    df = df.reset_index()\n",
    "    # df.drop('urban_rural', axis=1, inplace=True)\n",
    "\n",
    "    # Make sure all block groups are available\n",
    "    # Since we used the no_water version, there are some locations that were excluded\n",
    "    full_gdf_bg = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)\n",
    "    missing_bg = full_gdf_bg[~full_gdf_bg['geoid20'].isin(df['geoid20'])]\n",
    "    # print(missing_bg['geoid20'])\n",
    "    temp_df = df.iloc[:len(missing_bg)].copy()\n",
    "\n",
    "    # assign average acreage for all missing block groups (distributing equally across area types)\n",
    "    temp_df[['UrbanArea','TownArea','RuralArea']] = full_gdf_bg['acres'].mean()/3\n",
    "    temp_df['geoid20'] = missing_bg['geoid20'].values\n",
    "    df = df.append(temp_df)\n",
    "\n",
    "    # Convert to acres\n",
    "    df[['RuralArea','TownArea','UrbanArea']] = df[['RuralArea','TownArea','UrbanArea']]/43560.0\n",
    "\n",
    "    df['Year'] = year\n",
    "    df.rename(columns={'geoid20': 'Geo'}, inplace=True)\n",
    "\n",
    "    return df[_df.columns]\n",
    "\n",
    "df_18 = get_unprotected_area('2018', _df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "df_50 = get_unprotected_area('2050', _df)\n",
    "df = df_18.append(df_50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>urban_rural</th>\n",
       "      <th>Geo</th>\n",
       "      <th>Year</th>\n",
       "      <th>UrbanArea</th>\n",
       "      <th>TownArea</th>\n",
       "      <th>RuralArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Geo, Year, UrbanArea, TownArea, RuralArea]\n",
       "Index: []"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[['UrbanArea','TownArea','RuralArea']].sum(axis=1) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "full_gdf_bg = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1502.6688120293613"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'bzone_urban-mixed-use_prop.csv'\n",
    "# This file contains the target proportion of households located in mixed-used neighborhoods in zone and is used in the CalculateUrbanMixMeasure module.\n",
    "\n",
    "_df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "# Not sure how to define this; from parcels aggregate to BG and consider any zone with employment and households as mixed use?\n",
    "\n",
    "\n",
    "def get_mixed_use(parcel_df, year, _df):\n",
    "    \n",
    "    # parcel_df = parcel_18.copy()\n",
    "    parcel_df['total_units'] = parcel_df['SFUNITS']+parcel_df['MFUNITS']\n",
    "    parcel_df['mixed_use'] = 0\n",
    "    parcel_df.loc[parcel_df['SFUNITS'] < parcel_df['total_units'], 'mixed_use'] = 1\n",
    "\n",
    "    # Calculate\n",
    "    parcel_df['wt_mixed_use'] = parcel_df['mixed_use']*parcel_df['total_units']\n",
    "\n",
    "    df = parcel_df.groupby('geoid20').sum()[['total_units','wt_mixed_use']].reset_index()\n",
    "    df['MixUseProp'] = df['wt_mixed_use']/df['total_units']\n",
    "    df['MixUseProp'].fillna(0, inplace=True)\n",
    "\n",
    "    df['Year'] = year\n",
    "    df.rename(columns={'geoid20': 'Geo'}, inplace=True)\n",
    "\n",
    "    return df[_df.columns]\n",
    "\n",
    "df_18 = get_mixed_use(parcel_18.copy(), '2018', _df)\n",
    "df_50 = get_mixed_use(parcel_50.copy(), '2050', _df)\n",
    "df = df_18.append(df_50)\n",
    "\n",
    "# df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "(pyodbc.ProgrammingError) ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL Server]Failed to initialize the Common Language Runtime (CLR) v4.0.30319 with HRESULT 0x80004001. You may fix the problem and try again later. (6511) (SQLExecDirectW)')\n[SQL: select *, Shape.STAsText() as geometry from regional_geographies_preferred_alternative]\n(Background on this error at: http://sqlalche.me/e/14/f405)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\summary\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1705\u001b[0m                     self.dialect.do_execute(\n\u001b[1;32m-> 1706\u001b[1;33m                         \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1707\u001b[0m                     )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\summary\\lib\\site-packages\\sqlalchemy\\engine\\default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m         \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProgrammingError\u001b[0m: ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL Server]Failed to initialize the Common Language Runtime (CLR) v4.0.30319 with HRESULT 0x80004001. You may fix the problem and try again later. (6511) (SQLExecDirectW)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-5fe297379d59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0m_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mrg_shp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_from_sde\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'regional_geographies_preferred_alternative'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_table\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-503383560926>\u001b[0m in \u001b[0;36mread_from_sde\u001b[1;34m(connection_string, feature_class_name, version, crs, is_table)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         df=pd.read_sql('select *, Shape.STAsText() as geometry from %s' % \n\u001b[1;32m---> 28\u001b[1;33m                    (feature_class_name), con=con)\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mcon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\summary\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m             \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m         )\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\summary\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mread_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize)\u001b[0m\n\u001b[0;32m   1306\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1308\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1309\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\summary\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;34m\"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1176\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnectable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1178\u001b[0m     def read_table(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\summary\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, statement, *multiparams, **params)\u001b[0m\n\u001b[0;32m   1188\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m                 \u001b[0m_EMPTY_EXECUTION_OPTS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1190\u001b[1;33m                 \u001b[0mfuture\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1191\u001b[0m             )\n\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\summary\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_exec_driver_sql\u001b[1;34m(self, statement, multiparams, params, execution_options, future)\u001b[0m\n\u001b[0;32m   1489\u001b[0m             \u001b[0mexecution_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1490\u001b[0m             \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1491\u001b[1;33m             \u001b[0mdistilled_parameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1492\u001b[0m         )\n\u001b[0;32m   1493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\summary\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1747\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m             self._handle_dbapi_exception(\n\u001b[1;32m-> 1749\u001b[1;33m                 \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m             )\n\u001b[0;32m   1751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\summary\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[0;32m   1928\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mshould_wrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1929\u001b[0m                 util.raise_(\n\u001b[1;32m-> 1930\u001b[1;33m                     \u001b[0msqlalchemy_exception\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1931\u001b[0m                 )\n\u001b[0;32m   1932\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\summary\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;31m# credit to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\summary\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1704\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1705\u001b[0m                     self.dialect.do_execute(\n\u001b[1;32m-> 1706\u001b[1;33m                         \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1707\u001b[0m                     )\n\u001b[0;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\summary\\lib\\site-packages\\sqlalchemy\\engine\\default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m         \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo_execute_no_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProgrammingError\u001b[0m: (pyodbc.ProgrammingError) ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL Server]Failed to initialize the Common Language Runtime (CLR) v4.0.30319 with HRESULT 0x80004001. You may fix the problem and try again later. (6511) (SQLExecDirectW)')\n[SQL: select *, Shape.STAsText() as geometry from regional_geographies_preferred_alternative]\n(Background on this error at: http://sqlalche.me/e/14/f405)"
     ]
    }
   ],
   "source": [
    "fname = 'bzone_urban-town_du_proportions.csv'\n",
    "# This file contains proportion of Single-Family, Multi-Family and Group Quarter dwelling units within the urban portion of the zone and is used in the AssignLocTypes module.\n",
    "\n",
    "# from parcels file with land use types associated\n",
    "\n",
    "\n",
    "_df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "_df\n",
    "\n",
    "rg_shp = read_from_sde(connection_string, 'regional_geographies_preferred_alternative', version, crs=crs, is_table=False)\n",
    "\n",
    "\n",
    "def get_du_proportions(rg_shp, _parcel_df, year, _df):\n",
    "    # Note that we convert results to INT so there will never be a partial result\n",
    "    # Either an area is majority urban or rural; floats were causing issues when running scenarios\n",
    "    gdf = gpd.sjoin(_parcel_df, rg_shp, how='left')\n",
    "\n",
    "    rg_dict = {'CitiesTowns': 'TownArea',\n",
    "                'Core': 'UrbanArea',\n",
    "                'UU': 'UrbanArea',\n",
    "                'Metro': 'UrbanArea',\n",
    "                'HCT': 'UrbanArea',\n",
    "                'Rural': 'RuralArea'}\n",
    "    gdf['urban_rural'] = gdf['rg_propose_pa'].map(rg_dict)\n",
    "\n",
    "    df = gdf.pivot_table(index='geoid20', columns='urban_rural', values=['MFUNITS','SFUNITS'], aggfunc='sum').fillna(0).reset_index()\n",
    "    \n",
    "    for unit_type in ['SFUNITS','MFUNITS']:\n",
    "        df['PropTown'+unit_type[0:2]+'DU'] = df[unit_type]['TownArea']/df[unit_type].sum(axis=1).fillna(0).astype('int')\n",
    "        df['PropUrban'+unit_type[0:2]+'DU'] = df[unit_type]['UrbanArea']/df[unit_type].sum(axis=1).fillna(0).astype('int')\n",
    "\n",
    "    ######################\n",
    "    # FIXME: setting this match format from RVMPO, but not sure why it doesn't work as it\n",
    "    ######################\n",
    "    # df['PropUrbanSFDU'] = (df['SFUNITS']['UrbanArea']/df['SFUNITS'].sum(axis=1)).fillna(0)\n",
    "    # df['PropUrbanSFDU'] = 1\n",
    "    # df['PropUrbanMFDU'] = 1\n",
    "    # df['PropTownSFDU'] = 0\n",
    "    # df['PropTownMFDU'] = 0\n",
    "    df['PropTownGQDU'] = 0\n",
    "    df['PropUrbanGQDU'] = 0\n",
    "    df['Year'] = year\n",
    "    df.rename(columns={'geoid20': 'Geo'}, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    df.columns = df.columns.droplevel(1)\n",
    "\n",
    "\n",
    "    return df[_df.columns]\n",
    "\n",
    "df_18 = get_du_proportions(rg_shp, parcel_18_gdf, '2018', _df)\n",
    "df_50 = get_du_proportions(rg_shp, parcel_50_gdf, '2050', _df)\n",
    "df = df_18.append(df_50)\n",
    "\n",
    "# df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r'C:\\Workspace\\VisionEval\\models\\VERSPM\\inputs\\bzone_urban-town_du_proportions.csv')\n",
    "# col_list = ['PropUrbanSFDU','PropUrbanMFDU','PropUrbanGQDU','PropTownSFDU','PropTownMFDU','PropTownGQDU']\n",
    "# df[col_list] = df[col_list].astype('int')\n",
    "# df.to_csv(r'C:\\Workspace\\VisionEval\\models\\VERSPM\\inputs\\bzone_urban-town_du_proportions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geo</th>\n",
       "      <th>Year</th>\n",
       "      <th>PropUrbanSFDU</th>\n",
       "      <th>PropUrbanMFDU</th>\n",
       "      <th>PropUrbanGQDU</th>\n",
       "      <th>PropTownSFDU</th>\n",
       "      <th>PropTownMFDU</th>\n",
       "      <th>PropTownGQDU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>530330001011</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>530330001012</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>530330001013</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>530330001021</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>530330001022</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>530619400021</td>\n",
       "      <td>2050</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>530619400022</td>\n",
       "      <td>2050</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>530619400023</td>\n",
       "      <td>2050</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>530619900020</td>\n",
       "      <td>2050</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923</th>\n",
       "      <td>530619901000</td>\n",
       "      <td>2050</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5848 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Geo  Year  PropUrbanSFDU  PropUrbanMFDU  PropUrbanGQDU  \\\n",
       "0     530330001011  2018              1              1              0   \n",
       "1     530330001012  2018              1              1              0   \n",
       "2     530330001013  2018              1              1              0   \n",
       "3     530330001021  2018              1              1              0   \n",
       "4     530330001022  2018              1              1              0   \n",
       "...            ...   ...            ...            ...            ...   \n",
       "2919  530619400021  2050              1              1              0   \n",
       "2920  530619400022  2050              1              1              0   \n",
       "2921  530619400023  2050              1              1              0   \n",
       "2922  530619900020  2050              1              1              0   \n",
       "2923  530619901000  2050              1              1              0   \n",
       "\n",
       "      PropTownSFDU  PropTownMFDU  PropTownGQDU  \n",
       "0                0             0             0  \n",
       "1                0             0             0  \n",
       "2                0             0             0  \n",
       "3                0             0             0  \n",
       "4                0             0             0  \n",
       "...            ...           ...           ...  \n",
       "2919             0             0             0  \n",
       "2920             0             0             0  \n",
       "2921             0             0             0  \n",
       "2922             0             0             0  \n",
       "2923             0             0             0  \n",
       "\n",
       "[5848 rows x 8 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rg_shp = read_from_sde(connection_string, 'regional_geographies_preferred_alternative', version, crs=crs, is_table=False)\n",
    "# _parcel_df = parcel_18_gdf.copy()\n",
    "# gdf = gpd.sjoin(_parcel_df, rg_shp, how='left')\n",
    "\n",
    "# rg_dict = {'CitiesTowns': 'TownArea',\n",
    "#             'Core': 'UrbanArea',\n",
    "#             'UU': 'UrbanArea',\n",
    "#             'Metro': 'UrbanArea',\n",
    "#             'HCT': 'UrbanArea',\n",
    "#             'Rural': 'RuralArea'}\n",
    "# gdf['urban_rural'] = gdf['rg_propose_pa'].map(rg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = gdf.pivot_table(index='geoid20', columns='urban_rural', values=['MFUNITS','SFUNITS'], aggfunc='sum').fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gdf = gpd.sjoin(_parcel_df, rg_shp, how='left')\n",
    "# _parcel_df = parcel_18_gdf.copy()\n",
    "# _parcel_df = _parcel_df[_parcel_df['PARCELID'] != 0] \n",
    "# gdf = gpd.sjoin(_parcel_df, rg_shp, how='left')\n",
    "# rg_dict = {'CitiesTowns': 'TownArea',\n",
    "#         'Core': 'UrbanArea',\n",
    "#         'UU': 'UrbanArea',\n",
    "#         'Metro': 'UrbanArea',\n",
    "#         'HCT': 'UrbanArea',\n",
    "#         'Rural': 'RuralArea'}\n",
    "# gdf['urban_rural'] = gdf['rg_propose_pa'].map(rg_dict)\n",
    "\n",
    "# df = gdf.pivot_table(index='geoid20', columns='urban_rural', values=['MFUNITS','SFUNITS'], aggfunc='sum').fillna(0).reset_index()\n",
    "\n",
    "# for lu_type in ['Rural','Town','Urban']:\n",
    "#     df['tot_'+lu_type] = df['MFUNITS'][lu_type+'Area']+df['SFUNITS'][lu_type+'Area']\n",
    "#     df['Prop'+lu_type+'SFDU'] = df['SFUNITS'][lu_type+'Area']/df['tot_'+lu_type]\n",
    "#     df['Prop'+lu_type+'MFDU'] = df['MFUNITS'][lu_type+'Area']/df['tot_'+lu_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite geo.csv to only include block groups with some dwelling units\n",
    "# missing_bg\n",
    "# missing_bg = [530330053041,530330228033,530619900020,530339901000,\n",
    "#             530350903001,530359901000,530530726035,530530729031,\n",
    "#             530530729072,530530729082,530610401004,530619900020,530619901000]\n",
    "# fname = 'geo.csv'\n",
    "# df = pd.read_csv(os.path.join(output_dir,'..\\defs',fname))\n",
    "# df = df[~df['Bzone'].isin(missing_bg)]\n",
    "# df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M Area\n",
    "Regional totals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Optional files, not sure if they need to be included with zeros or not\n",
    "\n",
    "######### FIXME: \n",
    "######### create empty files with na values based on existing\n",
    "#########\n",
    "######### do something like load the existing file, fill with our region name and model years, and add empty null values\n",
    "\n",
    "'marea_base_year_dvmt.csv'\n",
    "# This file is used to specify to adjust the dvmt growth factors and is optional (only needed if user wants to modify the values). The file is used in the Initialize (VETravelPerformance), CalculateBaseRoadDvmt and CalculateFutureRoadDvmt modules.\n",
    "\n",
    "'marea_congestion_charges.csv' \n",
    "# This file is used to specify the charges of vehicle travel for different congestion levels and is optional. The file is used in the Initialize (VETravelPerformance) and CalculateRoadPerformance modules.\n",
    "\n",
    "'marea_dvmt_split_by_road_class.csv' \n",
    "# This file is used to specify the dvmt split for different road classes and is optional. The file is used in the Initialize (VETravelPerformance) and CalculateBaseRoadDvmt modules.\n",
    "\n",
    "'marea_operations_deployment.csv' \n",
    "# This file is used to specify the proportion of dvmt affected by operations for different road classes and is optional. The file is used in the Initialize (VETravelPerformance) and CalculateRoadPerformance modules.\n",
    "\n",
    "'marea_transit_ave_fuel_carbon_intensity.csv' \n",
    "# This file is used to specify the average carbon intensity of fuel used by transit and is optional. The file is used in the Initialize (VETravelPerformance) module.\n",
    "\n",
    "'marea_transit_biofuel_mix.csv' \n",
    "#This file is used to specify the biofuel used by transit and is optional. The file is used in the Initialize (VETravelPerformance) and CalculateCarbonIntensity modules.\n",
    "\n",
    "'marea_transit_fuel.csv' \n",
    "# This file is used to specify the transit fuel proportions and is optional. The file is used in the Initialize (VETravelPerformance) and CalculateCarbonIntensity modules.\n",
    "\n",
    "'marea_transit_powertrain_prop.csv' \n",
    "# This file is used to specify the mixes of transit vehicle powertrains and is optional. The file is used in the Initialize (VETravelPerformance) and CalculatePtranEnergyAndEmissions modules.\n",
    "\n",
    "for fname in ['marea_transit_powertrain_prop.csv',\n",
    "            'marea_base_year_dvmt.csv',\n",
    "            'marea_congestion_charges.csv',\n",
    "            'marea_dvmt_split_by_road_class.csv',\n",
    "            'marea_operations_deployment.csv',\n",
    "            'marea_transit_ave_fuel_carbon_intensity.csv',\n",
    "            'marea_transit_biofuel_mix.csv',\n",
    "            'marea_transit_fuel.csv',\n",
    "            'marea_transit_powertrain_prop.csv']:\n",
    "    df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "    # print(fname)\n",
    "    df['Geo'] = regional_geo\n",
    "    if 'Year' in df.columns:\n",
    "        df['Year'] = df['Year'].replace({2010: 2018, 2038: 2050})\n",
    "    df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "fname = 'marea_lane_miles.csv' \n",
    "# This file contains inputs on the numbers of freeway lane-miles and arterial lane-miles and is used in the AssignRoadMiles module.\n",
    "\n",
    "# FwyLaneMi: Lane-miles of roadways functionally classified as freeways or expressways in the urbanized portion of the metropolitan area\n",
    "# ArtLaneMi: Lane-miles of roadways functionally classified as arterials (but not freeways or expressways) in the urbanized portion of the metropolitan area\n",
    "\n",
    "def load_network_summary(filepath):\n",
    "    \"\"\"Load network-level results using a standard procedure. \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Congested network components by time of day\n",
    "    df.columns\n",
    "\n",
    "    # Get freeflow from 20to5 period\n",
    "\n",
    "    # Exclude trips taken on non-designated facilities (facility_type == 0)\n",
    "    # These are artificial (weave lanes to connect HOV) or for non-auto uses \n",
    "    df = df[df['data3'] != 0]    # data3 represents facility_type\n",
    "\n",
    "    # calculate total link VMT and VHT\n",
    "    df['VMT'] = df['@tveh']*df['length']\n",
    "    df['VHT'] = df['@tveh']*df['auto_time']/60\n",
    "\n",
    "    # Define facility type\n",
    "    df.loc[df['data3'].isin([1,2]), 'facility_type'] = 'highway'\n",
    "    df.loc[df['data3'].isin([3,4,6]), 'facility_type'] = 'arterial'\n",
    "    df.loc[df['data3'].isin([5]), 'facility_type'] = 'connector'\n",
    "\n",
    "    # Calculate delay\n",
    "    # Select links from overnight time of day\n",
    "    delay_df = df.loc[df['tod'] == '20to5'][['ij','auto_time']]\n",
    "    delay_df.rename(columns={'auto_time':'freeflow_time'}, inplace=True)\n",
    "\n",
    "    # Merge delay field back onto network link df\n",
    "    df = pd.merge(df, delay_df, on='ij', how='left')\n",
    "\n",
    "    # Calcualte hourly delay\n",
    "    df['total_delay'] = ((df['auto_time']-df['freeflow_time'])*df['@tveh'])/60    # sum of (volume)*(travtime diff from freeflow)\n",
    "\n",
    "    df['county'] =df['@countyid'].map({33: 'King',\n",
    "                                      35: 'Kitsap',\n",
    "                                      53: 'Pierce',\n",
    "                                      61: 'Snohomish'})\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_lane_miles(year, run_dir):\n",
    "    \"\"\"Note: only want lane miles iwthin URBANZINED PORTION\"\"\"\n",
    "    df_network = load_network_summary(os.path.join(run_dir,r'outputs\\network','network_results.csv'))\n",
    "    # Select mid-day network\n",
    "    gdf = df_network[df_network['tod'] == '10to14']\n",
    "    gdf['Lane Miles'] = gdf['length']*gdf['num_lanes']\n",
    "\n",
    "    ul3_dict = {\n",
    "        0: 'Rail/Walk/Ferry',\n",
    "        1: 'Freeway',\n",
    "        2: 'Expressway',\n",
    "        3: 'Urban Arterial',\n",
    "        4: 'One-way Arterial',\n",
    "        5: 'Centroid Connector',\n",
    "        6: 'Rural Arterial'\n",
    "    }\n",
    "\n",
    "    gdf['Facility Group'] = gdf['data3'].map(ul3_dict)\n",
    "    df = gdf.groupby(['Facility Group','data3']).sum()[['Lane Miles']].sort_values('data3').reset_index()\n",
    "\n",
    "    _df = pd.DataFrame([df[df['Facility Group'].isin(['Freeway','Expressway'])]['Lane Miles'].sum(),\n",
    "                    df[df['Facility Group'].isin(['Urban Arterial','Rural Arterial', 'One-way arterial'])]['Lane Miles'].sum()]).T\n",
    "    _df.columns = ['FwyLaneMi', 'ArtLaneMi']\n",
    "    _df['Year'] = year\n",
    "    \n",
    "    _df['Geo'] = regional_geo\n",
    "    \n",
    "    return _df\n",
    "\n",
    "regional_geo = 'PSRC'\n",
    "df18 = get_lane_miles('2018', r'L:\\RTP_2022\\final_runs\\sc_2018_rtp_final\\soundcast')\n",
    "df50 = get_lane_miles('2050', r'L:\\RTP_2022\\final_runs\\sc_rtp_2050_constrained_final\\soundcast')\n",
    "\n",
    "df = df18.append(df50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'marea_speed_smooth_ecodrive.csv'\n",
    "# This input file supplies information of deployment of speed smoothing and ecodriving by road class and vehicle type and is used in the CalculateMpgMpkwhAdjustments module.\n",
    "\n",
    "# Set to zero for now\n",
    "# FwySmooth:Fractional deployment of speed smoothing traffic management on freeways, where 0 is no deployment and 1 is the full potential fuel savings\n",
    "# ArtSmooth: Fractional deployment of speed smoothing traffic management on arterials, where 0 is no deployment and 1 is the full potential fuel savings\n",
    "# LdvEcoDrive: Eco-driving penetration for light-duty vehicles; the fraction of vehicles from 0 to 1\n",
    "# HvyTrkEcoDrive: Eco-driving penetration for heavy-duty vehicles; the fraction of vehicles from 0 to 1\n",
    "\n",
    "df = pd.DataFrame([0,0,0,0]).T\n",
    "df.columns = ['FwySmooth','ArtSmooth','LdvEcoDrive','HvyTrkEcoDrive']\n",
    "df['Year'] = '2018'\n",
    "df['Geo'] = regional_geo\n",
    "\n",
    "\n",
    "df50 = pd.DataFrame([0,0,0,0]).T\n",
    "df50.columns = ['FwySmooth','ArtSmooth','LdvEcoDrive','HvyTrkEcoDrive']\n",
    "df50['Year'] = '2050'\n",
    "df50['Geo'] = regional_geo\n",
    "\n",
    "df = df.append(df50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>route_id</th>\n",
       "      <th>INode</th>\n",
       "      <th>JNode</th>\n",
       "      <th>order</th>\n",
       "      <th>stop_number</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>length</th>\n",
       "      <th>ttf</th>\n",
       "      <th>stop_to_stop_distance</th>\n",
       "      <th>loop_index</th>\n",
       "      <th>seg_id</th>\n",
       "      <th>ij</th>\n",
       "      <th>transit_mode</th>\n",
       "      <th>LineID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>118002</td>\n",
       "      <td>112850</td>\n",
       "      <td>112846</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>112850</td>\n",
       "      <td>112846</td>\n",
       "      <td>0.075916</td>\n",
       "      <td>11</td>\n",
       "      <td>0.321659</td>\n",
       "      <td>1</td>\n",
       "      <td>118002-112850-112846</td>\n",
       "      <td>112850-112846</td>\n",
       "      <td>b</td>\n",
       "      <td>118002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>118002</td>\n",
       "      <td>112846</td>\n",
       "      <td>112765</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112846</td>\n",
       "      <td>112765</td>\n",
       "      <td>0.010751</td>\n",
       "      <td>11</td>\n",
       "      <td>0.321659</td>\n",
       "      <td>1</td>\n",
       "      <td>118002-112846-112765</td>\n",
       "      <td>112846-112765</td>\n",
       "      <td>b</td>\n",
       "      <td>118002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>118002</td>\n",
       "      <td>112765</td>\n",
       "      <td>112651</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112765</td>\n",
       "      <td>112651</td>\n",
       "      <td>0.019321</td>\n",
       "      <td>11</td>\n",
       "      <td>0.321659</td>\n",
       "      <td>1</td>\n",
       "      <td>118002-112765-112651</td>\n",
       "      <td>112765-112651</td>\n",
       "      <td>b</td>\n",
       "      <td>118002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>118002</td>\n",
       "      <td>112651</td>\n",
       "      <td>111598</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112651</td>\n",
       "      <td>111598</td>\n",
       "      <td>0.151089</td>\n",
       "      <td>11</td>\n",
       "      <td>0.321659</td>\n",
       "      <td>1</td>\n",
       "      <td>118002-112651-111598</td>\n",
       "      <td>112651-111598</td>\n",
       "      <td>b</td>\n",
       "      <td>118002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>118002</td>\n",
       "      <td>111598</td>\n",
       "      <td>111608</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>111598</td>\n",
       "      <td>111608</td>\n",
       "      <td>0.064581</td>\n",
       "      <td>11</td>\n",
       "      <td>0.321659</td>\n",
       "      <td>1</td>\n",
       "      <td>118002-111598-111608</td>\n",
       "      <td>111598-111608</td>\n",
       "      <td>b</td>\n",
       "      <td>118002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>134</td>\n",
       "      <td>118002</td>\n",
       "      <td>65169</td>\n",
       "      <td>65177</td>\n",
       "      <td>135</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>65169</td>\n",
       "      <td>65177</td>\n",
       "      <td>0.063525</td>\n",
       "      <td>11</td>\n",
       "      <td>0.246699</td>\n",
       "      <td>1</td>\n",
       "      <td>118002-65169-65177</td>\n",
       "      <td>65169-65177</td>\n",
       "      <td>b</td>\n",
       "      <td>118002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "      <td>118002</td>\n",
       "      <td>65177</td>\n",
       "      <td>65188</td>\n",
       "      <td>136</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>65177</td>\n",
       "      <td>65188</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>11</td>\n",
       "      <td>0.246699</td>\n",
       "      <td>1</td>\n",
       "      <td>118002-65177-65188</td>\n",
       "      <td>65177-65188</td>\n",
       "      <td>b</td>\n",
       "      <td>118002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>136</td>\n",
       "      <td>118002</td>\n",
       "      <td>65188</td>\n",
       "      <td>65222</td>\n",
       "      <td>137</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>65188</td>\n",
       "      <td>65222</td>\n",
       "      <td>0.123874</td>\n",
       "      <td>11</td>\n",
       "      <td>0.246699</td>\n",
       "      <td>1</td>\n",
       "      <td>118002-65188-65222</td>\n",
       "      <td>65188-65222</td>\n",
       "      <td>b</td>\n",
       "      <td>118002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>137</td>\n",
       "      <td>118002</td>\n",
       "      <td>65222</td>\n",
       "      <td>65372</td>\n",
       "      <td>138</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>65222</td>\n",
       "      <td>65372</td>\n",
       "      <td>0.128811</td>\n",
       "      <td>11</td>\n",
       "      <td>0.166294</td>\n",
       "      <td>1</td>\n",
       "      <td>118002-65222-65372</td>\n",
       "      <td>65222-65372</td>\n",
       "      <td>b</td>\n",
       "      <td>118002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>138</td>\n",
       "      <td>118002</td>\n",
       "      <td>65372</td>\n",
       "      <td>197918</td>\n",
       "      <td>139</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>65372</td>\n",
       "      <td>197918</td>\n",
       "      <td>0.037483</td>\n",
       "      <td>11</td>\n",
       "      <td>0.166294</td>\n",
       "      <td>1</td>\n",
       "      <td>118002-65372-197918</td>\n",
       "      <td>65372-197918</td>\n",
       "      <td>b</td>\n",
       "      <td>118002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>139 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  route_id   INode   JNode  order  stop_number  is_stop  \\\n",
       "0             0    118002  112850  112846      1            1        1   \n",
       "1             1    118002  112846  112765      2            1        0   \n",
       "2             2    118002  112765  112651      3            1        0   \n",
       "3             3    118002  112651  111598      4            1        0   \n",
       "4             4    118002  111598  111608      5            1        0   \n",
       "..          ...       ...     ...     ...    ...          ...      ...   \n",
       "134         134    118002   65169   65177    135           55        1   \n",
       "135         135    118002   65177   65188    136           55        0   \n",
       "136         136    118002   65188   65222    137           55        0   \n",
       "137         137    118002   65222   65372    138           56        1   \n",
       "138         138    118002   65372  197918    139           56        0   \n",
       "\n",
       "          i       j    length  ttf  stop_to_stop_distance  loop_index  \\\n",
       "0    112850  112846  0.075916   11               0.321659           1   \n",
       "1    112846  112765  0.010751   11               0.321659           1   \n",
       "2    112765  112651  0.019321   11               0.321659           1   \n",
       "3    112651  111598  0.151089   11               0.321659           1   \n",
       "4    111598  111608  0.064581   11               0.321659           1   \n",
       "..      ...     ...       ...  ...                    ...         ...   \n",
       "134   65169   65177  0.063525   11               0.246699           1   \n",
       "135   65177   65188  0.059300   11               0.246699           1   \n",
       "136   65188   65222  0.123874   11               0.246699           1   \n",
       "137   65222   65372  0.128811   11               0.166294           1   \n",
       "138   65372  197918  0.037483   11               0.166294           1   \n",
       "\n",
       "                   seg_id             ij transit_mode  LineID  \n",
       "0    118002-112850-112846  112850-112846            b  118002  \n",
       "1    118002-112846-112765  112846-112765            b  118002  \n",
       "2    118002-112765-112651  112765-112651            b  118002  \n",
       "3    118002-112651-111598  112651-111598            b  118002  \n",
       "4    118002-111598-111608  111598-111608            b  118002  \n",
       "..                    ...            ...          ...     ...  \n",
       "134    118002-65169-65177    65169-65177            b  118002  \n",
       "135    118002-65177-65188    65177-65188            b  118002  \n",
       "136    118002-65188-65222    65188-65222            b  118002  \n",
       "137    118002-65222-65372    65222-65372            b  118002  \n",
       "138   118002-65372-197918   65372-197918            b  118002  \n",
       "\n",
       "[139 rows x 17 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(os.path.join(run_dir,r'inputs\\scenario\\networks\\shapefiles\\AM\\AM_transit_segments.csv'))\n",
    "# df[df['LineID'] == 118002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'marea_transit_service.csv' \n",
    "# This file contains annual revenue-miles for different transit modes for metropolitan area and is used in the AssignTransitService module.\n",
    "\n",
    "# Get from Stefan's transit tool\n",
    "# import transit_service_analystk as tsa\n",
    "\n",
    "# path = r'R:/e2projects_two/Angela/transit_routes_2018/latest_combined'\n",
    "# transit_analyst = tsa.load_gtfs(path, '20180417', 0, 1680)\n",
    "\n",
    "# geo_df = transit_analyst.get_lines_gdf()\n",
    "\n",
    "# DRRevMi: Annual revenue-miles of demand-responsive public transit service\n",
    "# VPRevMi: Annual revenue-miles of van-pool and similar public transit service\n",
    "# MBRevMi: Annual revenue-miles of standard bus public transit service\n",
    "# RBRevMi: Annual revenue-miles of rapid-bus and commuter bus public transit service\n",
    "# MGRevMi: Annual revenue-miles of monorail and automated guideway public transit service\n",
    "# SRRevMi: Annual revenue-miles of streetcar and trolleybus public transit service\n",
    "# HRRevMi: Annual revenue-miles of light rail and heavy rail public transit service\n",
    "# CRRevMi: Annual revenue-miles of commuter rail, hybrid rail, cable car, and aerial tramway public transit service\n",
    "\n",
    "# GTFS Routes Types:\n",
    "# 0 - Tram, Streetcar, Light rail. Any light rail or street level system within a metropolitan area.\n",
    "# 1 - Subway, Metro. Any underground rail system within a metropolitan area.\n",
    "# 2 - Rail. Used for intercity or long-distance travel.\n",
    "# 3 - Bus. Used for short- and long-distance bus routes.\n",
    "# 4 - Ferry. Used for short- and long-distance boat service.\n",
    "# 5 - Cable tram. Used for street-level rail cars where the cable runs beneath the vehicle, e.g., cable car in San Francisco.\n",
    "# 6 - Aerial lift, suspended cable car (e.g., gondola lift, aerial tramway). Cable transport where cabins, cars, gondolas or open chairs are suspended by means of one or more cables.\n",
    "# 7 - Funicular. Any rail system designed for steep inclines.\n",
    "# 11 - Trolleybus. Electric buses that draw power from overhead wires using poles.\n",
    "# 12 - Monorail. Railway in which the track consists of a single rail or a beam.\n",
    "\n",
    "#########################\n",
    "\n",
    "# FIXME! need to be scaled up to get revenue vehicle miles; on the order of 60+M for Kc Metro\n",
    "# https://www.transit.dot.gov/sites/fta.dot.gov/files/transit_agency_profile_doc/2018/00010-00001.pdf\n",
    "\n",
    "# FIXME! how to code ferry? coding as catch-all commuter/hybrid rail plus cable car and aerial tramway\n",
    "\n",
    "# FIXME! get a GTFS for future year or pull form network?\n",
    "\n",
    "#########################\n",
    "\n",
    "# route_type_map = {\n",
    "#     0: 'HRRevMi',   # streetcar/light rail -> light/heavy rail\n",
    "#     1: 'HRRevMi',   # subway/metro -> light/heavy rail\n",
    "#     2: 'CRRevMi',   # commuter rail plus cable car, aerial tram\n",
    "#     3: 'MBRevMi',   # bus -> standard bus\n",
    "#     4: 'CRRevMi',  # ferry -> commuter rail plus cable car, aerial tram\n",
    "#     5: 'CRRevMi',   # cable tram ->  ' '\n",
    "#     6: 'CRRevMi',   # aerial lift -> ' '\n",
    "#     7: 'CRRevMi',   # funicular -> ' '\n",
    "#     11: 'SRRevMi',  # trolleybus -> streetcar and trolleybus service\n",
    "#     12: 'MGRevMi',  # monorail\n",
    "# }\n",
    "\n",
    "# annualize weekday service?\n",
    "annual_factor = 300\n",
    "\n",
    "# # Get revenue miles by multiplying daily vehicles times miles traveled\n",
    "# run_dir = r'C:\\Workspace\\sc_2018_rtp_final\\soundcast'\n",
    "# df_transit = gpd.read_file(os.path.join(run_dir,r'inputs\\scenario\\networks\\shapefiles\\AM\\AM_edges.shp'))\n",
    "\n",
    "# df = pd.read_csv(os.path.join(run_dir,r'inputs\\scenario\\networks\\shapefiles\\AM\\AM_transit_segments.csv'))\n",
    "# df_headways = pd.read_csv(os.path.join(run_dir,r'inputs\\scenario\\networks\\headways.csv'))\n",
    "# hdwy_cols = df_headways.columns.drop(['Unnamed: 0','LineID','id'])\n",
    "# # df.loc[df, 'daily_vehicles'](60.0/df_headways[hdwy_cols])\n",
    "# df_headways.index = df_headways.LineID\n",
    "# _df = (60.0/df_headways[hdwy_cols])\n",
    "# _df = _df.replace(np.inf, 0)\n",
    "# _df = _df.sum(axis=1)\n",
    "# _df = pd.DataFrame(_df, columns=['daily_vehicles'])\n",
    "# # _df = _df.reset_index()\n",
    "# df = df.merge(_df, left_on='LineID', right_index=True)\n",
    "\n",
    "# df_transit = df_transit.merge(df[['LineID','daily_vehicles','ij']], left_on='id', right_on='ij')\n",
    "\n",
    "# df\n",
    "\n",
    "# def get_transit_miles(geo_df, fname, year):\n",
    "#     geo_df['new_route_type'] = geo_df['route_type'].map(route_type_map)\n",
    "#     _df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "#     geo_df = geo_df.to_crs(crs)\n",
    "#     geo_df['miles'] = geo_df.length/5280.0\n",
    "#     df = geo_df.groupby('new_route_type').sum()[['miles']].T\n",
    "#     df = df*annual_factor\n",
    "#     for col in ['DRRevMi','VPRevMi','MBRevMi','RBRevMi','MGRevMi','SRRevMi','HRRevMi','CRRevMi']:\n",
    "#         if col not in df.columns:\n",
    "#             df[col] = 0\n",
    "#     df['Year'] = year\n",
    "#     df['Geo'] = regional_geo\n",
    "\n",
    "#     return df\n",
    "\n",
    "# df_18 = get_transit_miles(geo_df, fname, '2018')\n",
    "# df_50 = get_transit_miles(geo_df, fname, '2050')\n",
    "# df = df_18.append(df_50)\n",
    "# df.to_csv(os.path.join(output_dir,fname), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40616952.37306619"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # df_transit.groupby('route_type').sum()[['miles']].T\n",
    "# df_length = df.groupby('LineID').sum()[['length']]\n",
    "# df = df_length.merge(_df, left_index=True, right_index=True)\n",
    "# df['revenue_miles'] = df['length']*df['daily_vehicles']\n",
    "# df['revenue_miles'].sum()*annual_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate total daily transit vehicles\n",
    "# # df_headways\n",
    "# hdwy_cols = df_headways.columns.drop(['Unnamed: 0','LineID','id'])\n",
    "# # df.loc[df, 'daily_vehicles'](60.0/df_headways[hdwy_cols])\n",
    "# df_headways.index = df_headways.LineID\n",
    "# _df = (60.0/df_headways[hdwy_cols])\n",
    "# _df = _df.replace(np.inf, 0)\n",
    "# _df = _df.sum(axis=1)\n",
    "\n",
    "# _df = pd.DataFrame(_df, columns=['daily_vehicles'])\n",
    "# _df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Optional ##########\n",
    "\n",
    "'other_ops_effectiveness.csv'\n",
    "# This file is used to specify the delay effects of operations in different road classes and is optional (only needed if user wants to modify the values). The file is used in the Initialize (VETravelPerformance) and CalculateRoadPerformance modules.\n",
    "\n",
    "'region_ave_fuel_carbon_intensity.csv'\n",
    "#This file is used to specify the average carbon density for different vehicle types and is optional (only needed if user wants to modify the values). The file is used in the Initialize (VETravelPerformance) and CalculateCarbonIntensity modules.\n",
    "\n",
    "'region_base_year_hvytrk_dvmt.csv'\n",
    "# This file is used to specify the heavy truck dvmt for base year and is optional. The file is used in the Initialize (VETravelPerformance), CalculateBaseRoadDvmt and CalculateFutureRoadDvmt modules.\n",
    "\n",
    "'region_carsvc_powertrain_prop.csv'\n",
    "# This file is used to specify the powertrain proportion of car services and is optional. The file is used in the Initialize (VETravelPerformance), AssignHhVehiclePowertrain and AdjustHhVehicleMpgMpkwh modules.\n",
    "\n",
    "'region_comsvc_powertrain_prop.csv'\n",
    "#This file is used to specify the powertrain proportion of commercial vehicles and is optional. The file is used in the Initialize (VEPowertrainsAndFuels) and CalculateComEnergyAndEmissions modules.\n",
    "\n",
    "'region_hvytrk_powertrain_prop.csv'\n",
    "#This file is used to specify the powertrain proportion of heavy duty trucks and is optional. The file is used in the Initialize (VEPowertrainsAndFuels) and CalculateComEnergyAndEmissions modules.\n",
    "\n",
    "for fname in ['other_ops_effectiveness.csv',\n",
    "            'region_ave_fuel_carbon_intensity.csv',\n",
    "            'region_base_year_hvytrk_dvmt.csv',\n",
    "            'region_carsvc_powertrain_prop.csv',\n",
    "            'region_comsvc_powertrain_prop.csv',\n",
    "            'region_hvytrk_powertrain_prop.csv']:\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "    except:\n",
    "        print(fname + ': file not available')\n",
    "        next\n",
    "        # print(fname)\n",
    "    df['Geo'] = regional_geo\n",
    "    if 'Year' in df.columns:\n",
    "        df['Year'] = df['Year'].replace({2010: 2018, 2038: 2050})\n",
    "    df.to_csv(os.path.join(output_dir,fname), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following regional levels inputs are required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_comsvc_lttrk_prop.csv'\n",
    "# This file supplies data for the light truck proportion of commercial vehicles and is used in the CalculateComEnergyAndEmissions module.\n",
    "\n",
    "########## FIXME: assuming 50% for now?\n",
    "\n",
    "\n",
    "# ComSvcLtTrkProp: Regional proportion of commercial service vehicles that are light trucks Here is a snapshot of the file:\n",
    "\n",
    "df = pd.DataFrame([0.5]).T\n",
    "df.columns = ['ComSvcLtTrkProp']\n",
    "df['Year'] = '2018'\n",
    "df['Geo'] = regional_geo\n",
    "\n",
    "\n",
    "df50 = pd.DataFrame([0.5]).T\n",
    "df50.columns = ['ComSvcLtTrkProp']\n",
    "df50['Year'] = '2050'\n",
    "df50['Geo'] = regional_geo\n",
    "\n",
    "df = df.append(df50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_hh_driver_adjust_prop.csv'\n",
    "#This file specifies the relative driver licensing rate relative to the model estimation data year and is used in the AssignDrivers module.\n",
    "\n",
    "## FIXME: set to 1\n",
    "\n",
    "# Drv15to19AdjProp: Target proportion of unadjusted model number of drivers 15 to 19 years old (1 = no adjustment)\n",
    "# Drv20to29AdjProp: Target proportion of unadjusted model number of drivers 20 to 29 years old (1 = no adjustment)\n",
    "# Drv30to54AdjProp: Target proportion of unadjusted model number of drivers 30 to 54 years old (1 = no adjustment)\n",
    "# Drv55to64AdjProp: Target proportion of unadjusted model number of drivers 55 to 64 years old (1 = no adjustment)\n",
    "# Drv65PlusAdjProp: Target proportion of unadjusted model number of drivers 65 or older (1 = no adjustment)\n",
    "\n",
    "\n",
    "df = pd.DataFrame([1,1,1,1,1]).T\n",
    "df.columns = ['Drv15to19AdjProp','Drv20to29AdjProp','Drv30to54AdjProp','Drv55to64AdjProp','Drv65PlusAdjProp']\n",
    "df['Year'] = '2018'\n",
    "df['Geo'] = regional_geo\n",
    "# df\n",
    "\n",
    "df50 = pd.DataFrame([1,1,1,1,1]).T\n",
    "df50.columns = ['Drv15to19AdjProp','Drv20to29AdjProp','Drv30to54AdjProp','Drv55to64AdjProp','Drv65PlusAdjProp']\n",
    "df50['Year'] = '2050'\n",
    "df50['Geo'] = regional_geo\n",
    "df_50\n",
    "df = df.append(df50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'region_prop_externalities_paid.csv'\n",
    "#This file supplies data for climate change and other social costs and is used in the CalculateVehicleOperatingCost module.\n",
    "\n",
    "# PropClimateCostPaid: Proportion of climate change costs paid by users (i.e. ratio of carbon taxes to climate change costs\n",
    "# PropOtherExtCostPaid: Proportion of other social costs paid by users\n",
    "\n",
    "df = pd.DataFrame([0,0]).T\n",
    "df.columns = ['PropClimateCostPaid','PropOtherExtCostPaid']\n",
    "df['Year'] = '2018'\n",
    "df['Geo'] = regional_geo\n",
    "# df\n",
    "\n",
    "df50 = pd.DataFrame([0,0]).T\n",
    "df50.columns = ['PropClimateCostPaid','PropOtherExtCostPaid']\n",
    "df50['Year'] = '2050'\n",
    "df50['Geo'] = regional_geo\n",
    "df_50\n",
    "df = df.append(df50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import filecmp\n",
    "import glob\n",
    "# print()\n",
    "files_target = glob.glob(r\"C:\\Workspace\\VisionEval\\models\\VERSPM\\inputs\\*\")\n",
    "files_new = glob.glob(r\"C:\\Workspace\\VisionEval\\input_creation\\psrc_inputs\\*\")\n",
    "# match, mismatch, errors = filecmp.cmpfiles('C:\\Workspace\\VisionEval\\input_creation\\psrc_inputs', 'C:\\Workspace\\VisionEval\\models\\VERSPM\\inputs', files)\n",
    "\n",
    "\n",
    "# Get rap file names only\n",
    "files_target = [i.split('\\\\')[-1] for i in files_target]\n",
    "files_new = [i.split('\\\\')[-1] for i in files_new]\n",
    "\n",
    "\n",
    "[i for i in files_target if i not in files_new]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the remaining missing files\n",
    "# Keeping default values from RVMPO for now\n",
    "\n",
    "fname = 'azone_hh_ave_veh_per_driver.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'azone_hh_lttrk_prop.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_base_year_dvmt.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "# df['Year'] = ['2018','2050']\n",
    "# df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_co2e_costs.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "# df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_comsvc_ave_veh_age.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "# df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_comsvc_veh_mean_age.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "# df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_hh_ave_driver_per_capita.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "# df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_road_cost.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "# df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "gdf_shp = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_shp.to_file(r'C:\\Workspace\\VisionEval\\models\\VERSPM\\blockgrp2020.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a01578ef7fc98460838ddfd60bd3288ea700197594b0894c527f4f3349251842"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('summary': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
