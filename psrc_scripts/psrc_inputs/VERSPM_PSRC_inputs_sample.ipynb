{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import geopandas as gpd\n",
    "import sqlalchemy\n",
    "from shapely import wkt\n",
    "import h5py\n",
    "import pandana as pdna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_sde(connection_string, feature_class_name, version,\n",
    "                  crs={'init': 'epsg:2285'}, is_table = False):\n",
    "    \"\"\"\n",
    "    Returns the specified feature class as a geodataframe from ElmerGeo.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    connection_string : SQL connection string that is read by geopandas \n",
    "                        read_sql function\n",
    "    \n",
    "    feature_class_name: the name of the featureclass in PSRC's ElmerGeo \n",
    "                        Geodatabase\n",
    "    \n",
    "    cs: cordinate system\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    engine = sqlalchemy.create_engine(connection_string)\n",
    "    con=engine.connect()\n",
    "    #con.execute(\"sde.set_current_version {0}\".format(version))\n",
    "    if is_table:\n",
    "        gdf=pd.read_sql('select * from %s' % \n",
    "                   (feature_class_name), con=con)\n",
    "        con.close()\n",
    "\n",
    "    else:\n",
    "        df=pd.read_sql('select *, Shape.STAsText() as geometry from %s' % \n",
    "                   (feature_class_name), con=con)\n",
    "        con.close()\n",
    "\n",
    "        df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "        gdf=gpd.GeoDataFrame(df, geometry='geometry')\n",
    "        gdf.crs = crs\n",
    "        cols = [col for col in gdf.columns if col not in \n",
    "                ['Shape', 'GDB_GEOMATTR_DATA', 'SDE_STATE_ID']]\n",
    "        gdf = gdf[cols]\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r'C:\\Workspace\\VisionEval\\models\\VERSPM\\inputs_RVMPO'\n",
    "output_dir = r'C:\\Workspace\\VisionEval\\models\\VERSPM\\inputs'\n",
    "\n",
    "regional_geo = 'PSRC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "# Load data that will be used for multiple fields\n",
    "gdf_bg = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)\n",
    "\n",
    "run_dir_18 = r'C:\\Workspace\\sc_2018_rtp_final\\soundcast'\n",
    "run_dir_50 = r'L:\\RTP_2022\\final_runs\\sc_rtp_2050_constrained_final\\soundcast'\n",
    "\n",
    "parcel_18 = pd.read_csv(os.path.join(run_dir_18,'inputs\\scenario\\landuse\\parcels_urbansim.txt'), delim_whitespace=True)\n",
    "parcel_50 = pd.read_csv(os.path.join(run_dir_50,'inputs\\scenario\\landuse\\parcels_urbansim.txt'), delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookup for parcels to block groups\n",
    "# Load parcel centroids as geodataframe\n",
    "parcel_18_gdf = gpd.GeoDataFrame(\n",
    "    parcel_18, geometry=gpd.points_from_xy(parcel_18.XCOORD_P, parcel_18.YCOORD_P))\n",
    "crs = {'init' : 'EPSG:2285'}\n",
    "parcel_18_gdf.crs = crs\n",
    "\n",
    "parcel_50_gdf = gpd.GeoDataFrame(\n",
    "    parcel_50, geometry=gpd.points_from_xy(parcel_50.XCOORD_P, parcel_50.YCOORD_P))\n",
    "crs = {'init' : 'EPSG:2285'}\n",
    "parcel_50_gdf.crs = crs\n",
    "\n",
    "parcel_18_bg_lookup = gpd.sjoin(gdf_bg, parcel_18_gdf)[['PARCELID','geoid20']]\n",
    "parcel_50_bg_lookup = gpd.sjoin(gdf_bg, parcel_50_gdf)[['PARCELID','geoid20']]\n",
    "\n",
    "parcel_18 = parcel_18.merge(parcel_18_bg_lookup, on='PARCELID')\n",
    "parcel_50 = parcel_50.merge(parcel_50_bg_lookup, on='PARCELID')\n",
    "\n",
    "# Add the census tract back to the gdf\n",
    "parcel_18_gdf = parcel_18_gdf.merge(parcel_18_bg_lookup, on='PARCELID')\n",
    "parcel_50_gdf = parcel_50_gdf.merge(parcel_50_bg_lookup, on='PARCELID')\n",
    "\n",
    "# parcel_18_gdf = fill_dummy_parcels(parcel_18_gdf, gdf_shp)\n",
    "# parcel_50_gdf = fill_dummy_parcels(parcel_50_gdf, gdf_shp)\n",
    "\n",
    "# def fill_dummy_parcels(parcel_df, gdf_shp):\n",
    "#     # Add some dummy variables for block groups that do not have any parcels \n",
    "#     diff_list = list(set(gdf_shp['geoid20'].unique()) - set(parcel_df['geoid20'].unique()))\n",
    "\n",
    "#     # Add empty rows that represent this geoid\n",
    "#     temp_df = parcel_df.iloc[0:len(diff_list)].copy()\n",
    "\n",
    "#     temp_df[temp_df.columns] = 0\n",
    "#     temp_df['geoid20'] = diff_list\n",
    "#     parcel_df = parcel_df.append(temp_df)\n",
    "\n",
    "#     return parcel_df\n",
    "\n",
    "# parcel_18 = fill_dummy_parcels(parcel_18, gdf_shp)\n",
    "# parcel_50 = fill_dummy_parcels(parcel_50, gdf_shp)\n",
    "\n",
    "# Give dummy parcels an XY coordinate at centroid of their block groups\n",
    "# block_group_list = parcel_18[parcel_18['PARCELID'] == 0]['geoid20']\n",
    "\n",
    "# parcel_18.loc[parcel_18['PARCELID'] == 0, 'XCOORD_P'] = gdf_shp[gdf_shp['geoid20'].isin(block_group_list)].centroid.x.values\n",
    "# parcel_18.loc[parcel_18['PARCELID'] == 0, 'YCOORD_P'] = gdf_shp[gdf_shp['geoid20'].isin(block_group_list)].centroid.y.values\n",
    "# parcel_18[parcel_18['PARCELID'] == 0]\n",
    "\n",
    "\n",
    "# # Give dummy parcels an XY coordinate at centroid of their block groups\n",
    "# block_group_list = parcel_50[parcel_50['PARCELID'] == 0]['geoid20']\n",
    "\n",
    "# parcel_50.loc[parcel_50['PARCELID'] == 0, 'XCOORD_P'] = gdf_shp[gdf_shp['geoid20'].isin(block_group_list)].centroid.x.values\n",
    "# parcel_50.loc[parcel_50['PARCELID'] == 0, 'YCOORD_P'] = gdf_shp[gdf_shp['geoid20'].isin(block_group_list)].centroid.y.values\n",
    "\n",
    "\n",
    "# # Recreate the gdf\n",
    "\n",
    "# parcel_18_gdf = gpd.GeoDataFrame(\n",
    "#     parcel_18, geometry=gpd.points_from_xy(parcel_18.XCOORD_P, parcel_18.YCOORD_P))\n",
    "# crs = {'init' : 'EPSG:2285'}\n",
    "# parcel_18_gdf.crs = crs\n",
    "\n",
    "# parcel_50_gdf = gpd.GeoDataFrame(\n",
    "#     parcel_50, geometry=gpd.points_from_xy(parcel_50.XCOORD_P, parcel_50.YCOORD_P))\n",
    "# crs = {'init' : 'EPSG:2285'}\n",
    "# parcel_50_gdf.crs = crs\n",
    "\n",
    "# parcel_18_gdf = parcel_18_gdf.merge(parcel_18_bg_lookup, on='PARCELID')\n",
    "# parcel_50_gdf = parcel_50_gdf.merge(parcel_50_bg_lookup, on='PARCELID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Azone</th>\n",
       "      <th>Bzone</th>\n",
       "      <th>Czone</th>\n",
       "      <th>Marea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RVMPO</td>\n",
       "      <td>D410290001001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RVMPO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RVMPO</td>\n",
       "      <td>D410290001002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RVMPO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RVMPO</td>\n",
       "      <td>D410290002011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RVMPO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RVMPO</td>\n",
       "      <td>D410290002012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RVMPO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RVMPO</td>\n",
       "      <td>D410290002013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RVMPO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Azone          Bzone  Czone  Marea\n",
       "0  RVMPO  D410290001001    NaN  RVMPO\n",
       "1  RVMPO  D410290001002    NaN  RVMPO\n",
       "2  RVMPO  D410290002011    NaN  RVMPO\n",
       "3  RVMPO  D410290002012    NaN  RVMPO\n",
       "4  RVMPO  D410290002013    NaN  RVMPO"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write an initial Geo file\n",
    "# Note that we only want to include zones that have households located in them\n",
    "fname = 'geo.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,'..\\defs',fname))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "# Define geogrpahy\n",
    "# For now make an exact replica fo the RVMPO data with Azone = region, Bzone = block groups\n",
    "# Ideally we have Azone = county, Bzone = block groups\n",
    "\n",
    "# Get Census tracts from the region\n",
    "# Load  tract geographies from ElmerGeo\n",
    "# NOTE: We are using 2020 geographies;\n",
    "# Use geoid20 as the field name\n",
    "connection_string = 'mssql+pyodbc://AWS-PROD-SQL\\Sockeye/ElmerGeo?driver=SQL Server?Trusted_Connection=yes'\n",
    "crs = {'init' : 'EPSG:2285'}\n",
    "version = \"'DBO.Default'\"\n",
    "gdf_shp = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)\n",
    "\n",
    "# Load cities shapefile\n",
    "gdf_cities = read_from_sde(connection_string, 'cities', version, crs=crs, is_table=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_seattle = gdf_cities[gdf_cities['city_name'] == 'Seattle']\n",
    "# geoid_list = gpd.sjoin(gdf_shp, gdf_seattle)['geoid20']\n",
    "\n",
    "# # \n",
    "# df_geog = gdf_shp[['geoid20']]\n",
    "# geoid_list = geoid_list[geoid_list != '530330053041']\n",
    "# ## For now let's just select a sub sample of Block groups in Seattle\n",
    "# df_geog = df_geog[df_geog['geoid20'].isin(geoid_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "geoid_list = ['530330028004','530330028001','530330028002','530330028003']\n",
    "df_geog = gdf_shp[['geoid20']]\n",
    "df_geog = df_geog[df_geog['geoid20'].isin(geoid_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Azone</th>\n",
       "      <th>Bzone</th>\n",
       "      <th>Czone</th>\n",
       "      <th>Marea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330028001</td>\n",
       "      <td>0</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330028002</td>\n",
       "      <td>1</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330028003</td>\n",
       "      <td>2</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>530330028004</td>\n",
       "      <td>3</td>\n",
       "      <td>PSRC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Azone         Bzone  Czone Marea\n",
       "97   PSRC  530330028001      0  PSRC\n",
       "98   PSRC  530330028002      1  PSRC\n",
       "99   PSRC  530330028003      2  PSRC\n",
       "100  PSRC  530330028004      3  PSRC"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_geog.rename(columns={'geoid20': 'Bzone'}, inplace=True)\n",
    "df_geog.loc[:,'Azone'] = 'PSRC'\n",
    "df_geog.loc[:,'Marea'] = 'PSRC'\n",
    "df_geog.loc[:,'Czone'] = range(len(df_geog))\n",
    "df_geog[df.columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_geog[df.columns].to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of parcels in the block group area\n",
    "\n",
    "geoid_list = ['530330028004','530330028001','530330028002','530330028003']\n",
    "# df_geog = gdf_shp[['geoid20']]\n",
    "temp_df_geog = gdf_shp[gdf_shp['geoid20'].isin(geoid_list)]\n",
    "\n",
    "parcel_list = gpd.sjoin(temp_df_geog, parcel_18_gdf)['PARCELID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geo</th>\n",
       "      <th>Year</th>\n",
       "      <th>HighCarSvcCost.2010</th>\n",
       "      <th>LowCarSvcCost.2010</th>\n",
       "      <th>AveCarSvcVehicleAge</th>\n",
       "      <th>LtTrkCarSvcSubProp</th>\n",
       "      <th>AutoCarSvcSubProp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>2050</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Geo  Year  HighCarSvcCost.2010  LowCarSvcCost.2010  AveCarSvcVehicleAge  \\\n",
       "0  PSRC  2018                    1                   3                    3   \n",
       "1  PSRC  2050                    1                   3                    2   \n",
       "\n",
       "   LtTrkCarSvcSubProp  AutoCarSvcSubProp  \n",
       "0                0.75                  1  \n",
       "1                0.75                  1  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_dir = r'C:\\Workspace\\VisionEval\\models\\VERSPM\\inputs'\n",
    "# output_dir = r'C:\\Workspace\\VisionEval\\input_creation\\psrc_inputs'\n",
    "\n",
    "fname = 'azone_carsvc_characteristics.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update A-Zone files\n",
    "Currently reprsenting as for full region\n",
    "According to docs, should be about the size of a PUMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'azone_charging_availability.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many of these we are not yet updating with region-specific numbers and are using the defaults\n",
    "# However, we need to change the region name in the column\n",
    "# additionally, we need to provide specific years of data\n",
    "# Using 2018 and 2050 for years now; we could specify all model years \n",
    "\n",
    "def update_azone(df, name):\n",
    "    df['Geo'] = name\n",
    "    df['Year'] = ['2018','2050']\n",
    "    return df\n",
    "\n",
    "def process_azone(fname):\n",
    "    df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "    df = update_azone(df, 'PSRC')\n",
    "    df.to_csv(os.path.join(output_dir,fname), index=False)\n",
    "\n",
    "\n",
    "# This file specifies the different characteristics for high and low car service level and is used in the \n",
    "# CreateVehicleTable and AssignVehicleAge modules.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_charging_availability.csv'\n",
    "# This file has data on proportion of different household types who has EV charging available and is used in the \n",
    "# AssignHHVehiclePowertrain module.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_electricity_carbon_intensity.csv'\n",
    "# This file is used to specify the carbon intensity of electricity and is optional (only needed if user wants to modify the values). \n",
    "# The file is used in Initialize (VEPowertrainsAndFuels) and CalculateCarbonIntensity modules.\n",
    "process_azone(fname)\n",
    "\n",
    "\n",
    "fname = 'azone_fuel_power_cost.csv'\n",
    "# This file supplies data for retail cost of fuel and electricity and is used in the CalculateVehicleOperatingCost module.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_hhsize_targets.csv' \n",
    "# This file contains the household specific targets and is used in CreateHouseholds module.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_hh_veh_mean_age.csv' \n",
    "# This file provides inputs for mean auto age and mean light truck age and is used in the AssignVehicleAge module.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_hh_veh_own_taxes.csv' \n",
    "# This file provides inputs for flat fees/taxes (i.e. annual cost per vehicle) and ad valorem taxes (i.e. percentage of vehicle value paid in taxes). The file is used in CalculateVehicleOwnCost module.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_payd_insurance_prop.csv' \n",
    "# This file provides inputs on the proportion of households having PAYD (pay-as-you-drive) insurance and is used in the CalculateVehicleOwnCost module.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_prop_sov_dvmt_diverted.csv' \n",
    "# This file provides inputs for a goal for diverting a portion of SOV travel within a 20-mile tour distance and is used in the DivertSovTravel module.\n",
    "process_azone(fname)\n",
    "\n",
    "# fname = 'azone_relative_employment.csv' \n",
    "# # This file contains ratio of workers to persons by age and is used in the PredictWorkers module.\n",
    "# process_azone(fname)\n",
    "\n",
    "fname = 'azone_veh_use_taxes.csv' \n",
    "# This file supplies data for vehicle related taxes and is used in the CalculateVehicleOperatingCost module.\n",
    "process_azone(fname)\n",
    "\n",
    "fname = 'azone_vehicle_access_times.csv' \n",
    "# This file supplies data for vehicle acces`s and egress time and is used in the CalculateVehicleOperatingCost module.\n",
    "process_azone(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'azone_gq_pop_by_age.csv' \n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "# This file contains group quarters population estimates/forecasts by age and is used in the CreateHouseholds module.\n",
    "\n",
    "# Get group quarters from parcel file\n",
    "\n",
    "# Taking group quarters populations to be Zero in Soundcast\n",
    "df[['GrpAge0to14','GrpAge15to19','GrpAge20to29','GrpAge30to54','GrpAge55to64','GrpAge65Plus']] = 0 \n",
    "df['Geo'] = regional_geo\n",
    "df['Year'] = ['2018','2050']\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)\n",
    "# parcel_18.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'azone_hh_pop_by_age.csv' \n",
    "# This file contains population estimates/forecasts by age and is used in the CreateHouseholds module.\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "# Get this data from synthetic household/persons\n",
    "\n",
    "\n",
    "def get_age_population(syn_h5, year):\n",
    "\n",
    "    df_person = pd.DataFrame()\n",
    "    for col in syn_h5['Person'].keys():\n",
    "        df_person[col] = syn_h5['Person'][col][:]\n",
    "\n",
    "    df_hh = pd.DataFrame()\n",
    "    for col in ['hhno','hhparcel']:\n",
    "        df_hh[col] = syn_h5['Household'][col][:]\n",
    "\n",
    "    df_person = df_person.merge(df_hh, how='left', on='hhno')\n",
    "    # Filter for people living in Seattle only\n",
    "    df_person = df_person[df_person['hhparcel'].isin(parcel_list)]\n",
    "    col_list = [i for i in df.columns if i not in ['Geo','Year']]\n",
    "\n",
    "    # Separate ages into groups\n",
    "    bins = [-1,14,19,29,54,64,200]\n",
    "    df_person['new_group'] = pd.cut(df_person['pagey'], bins, labels=col_list)\n",
    "\n",
    "    _df = df_person.groupby('new_group').count()[['psexpfac']].reset_index().T\n",
    "    _df = _df.reset_index(drop=True)\n",
    "    _df.columns = _df.iloc[0]\n",
    "    _df = _df.iloc[1:]\n",
    "    # _df.drop('new_group', axis=1, inplace=True)\n",
    "    _df['Year'] = year\n",
    "    _df['Geo'] = 'PSRC'\n",
    "    _df = _df.reset_index(drop=True)\n",
    "    \n",
    "    return _df\n",
    "\n",
    "syn_h5 = h5py.File(r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2018\\new_emp\\hh_and_persons.h5', 'r')\n",
    "_df_18 = get_age_population(syn_h5, '2018')\n",
    "_df_18 = _df_18[df.columns]\n",
    "syn_h5.close()\n",
    "\n",
    "syn_h5 = h5py.File(r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2050\\lodes\\hh_and_persons.h5', 'r')\n",
    "_df_50 = get_age_population(syn_h5, '2050')\n",
    "_df_50 = _df_50[df.columns]\n",
    "syn_h5.close()\n",
    "\n",
    "_df = _df_18.append(_df_50)\n",
    "_df.to_csv(os.path.join(output_dir,fname), index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geo</th>\n",
       "      <th>Year</th>\n",
       "      <th>Age0to14</th>\n",
       "      <th>Age15to19</th>\n",
       "      <th>Age20to29</th>\n",
       "      <th>Age30to54</th>\n",
       "      <th>Age55to64</th>\n",
       "      <th>Age65Plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>2018</td>\n",
       "      <td>779</td>\n",
       "      <td>192</td>\n",
       "      <td>706</td>\n",
       "      <td>2033</td>\n",
       "      <td>524</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PSRC</td>\n",
       "      <td>2050</td>\n",
       "      <td>909</td>\n",
       "      <td>274</td>\n",
       "      <td>684</td>\n",
       "      <td>1731</td>\n",
       "      <td>841</td>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0   Geo  Year Age0to14 Age15to19 Age20to29 Age30to54 Age55to64 Age65Plus\n",
       "0  PSRC  2018      779       192       706      2033       524       489\n",
       "0  PSRC  2050      909       274       684      1731       841       873"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syn_h5 = h5py.File(r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2018\\new_emp\\hh_and_persons.h5', 'r')\n",
    "\n",
    "# df_person = pd.DataFrame()\n",
    "# for col in syn_h5['Person'].keys():\n",
    "#     df_person[col] = syn_h5['Person'][col][:]\n",
    "\n",
    "# df_hh = pd.DataFrame()\n",
    "# for col in ['hhno','hhparcel']:\n",
    "#     df_hh[col] = syn_h5['Household'][col][:]\n",
    "\n",
    "# df_person = df_person.merge(df_hh, how='left', on='hhno')\n",
    "# # Filter for people living in Seattle only\n",
    "\n",
    "# # df_person = df_person[df_person['hhparcel'].isin(parcel_list)]\n",
    "# # col_list = [i for i in df.columns if i not in ['Geo','Year']]\n",
    "_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hhno</th>\n",
       "      <th>pagey</th>\n",
       "      <th>pdairy</th>\n",
       "      <th>pgend</th>\n",
       "      <th>pno</th>\n",
       "      <th>ppaidprk</th>\n",
       "      <th>pptyp</th>\n",
       "      <th>prace</th>\n",
       "      <th>psexpfac</th>\n",
       "      <th>pspcl</th>\n",
       "      <th>pstaz</th>\n",
       "      <th>pstyp</th>\n",
       "      <th>ptpass</th>\n",
       "      <th>puwarrp</th>\n",
       "      <th>puwdepp</th>\n",
       "      <th>puwmode</th>\n",
       "      <th>pwpcl</th>\n",
       "      <th>pwtaz</th>\n",
       "      <th>pwtyp</th>\n",
       "      <th>hhparcel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>371473</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>64747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>371473</td>\n",
       "      <td>33</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>64747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>371473</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>64747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>327405</td>\n",
       "      <td>67</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>413627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>301717</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>552382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021852</th>\n",
       "      <td>405145</td>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>538473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024129</th>\n",
       "      <td>314054</td>\n",
       "      <td>83</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>219352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024130</th>\n",
       "      <td>314054</td>\n",
       "      <td>76</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>219352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025387</th>\n",
       "      <td>158214</td>\n",
       "      <td>32</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>242547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025388</th>\n",
       "      <td>158214</td>\n",
       "      <td>27</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>242547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>699968 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           hhno  pagey  pdairy  pgend  pno  ppaidprk  pptyp  prace  psexpfac  \\\n",
       "1312     371473     25      -1     -1    1        -1      5      1         1   \n",
       "1319     371473     33      -1      2    2        -1      1      1         1   \n",
       "1651     371473      1      -1     -1    3        -1      8     -1         1   \n",
       "1652     327405     67      -1      2    1        -1      3      1         1   \n",
       "1836     301717     26      -1      1    1        -1      1      4         1   \n",
       "...         ...    ...     ...    ...  ...       ...    ...    ...       ...   \n",
       "4021852  405145     70      -1      1    1        -1      3      1         1   \n",
       "4024129  314054     83      -1      1    1        -1      3      1         1   \n",
       "4024130  314054     76      -1      2    2        -1      3      1         1   \n",
       "4025387  158214     32      -1      1    2        -1      4      1         1   \n",
       "4025388  158214     27      -1      2    1        -1      4      1         1   \n",
       "\n",
       "         pspcl  pstaz  pstyp  ptpass  puwarrp  puwdepp  puwmode  pwpcl  pwtaz  \\\n",
       "1312        -1     -1      1      -1       -1       -1       -1     -1     -1   \n",
       "1319        -1     -1      0      -1       -1       -1       -1     -1     -1   \n",
       "1651        -1     -1      0      -1       -1       -1       -1     -1     -1   \n",
       "1652        -1     -1      0      -1       -1       -1       -1     -1     -1   \n",
       "1836        -1     -1      0      -1       -1       -1       -1     -1     -1   \n",
       "...        ...    ...    ...     ...      ...      ...      ...    ...    ...   \n",
       "4021852     -1     -1      0      -1       -1       -1       -1     -1     -1   \n",
       "4024129     -1     -1      0      -1       -1       -1       -1     -1     -1   \n",
       "4024130     -1     -1      0      -1       -1       -1       -1     -1     -1   \n",
       "4025387     -1     -1      0      -1       -1       -1       -1     -1     -1   \n",
       "4025388     -1     -1      0      -1       -1       -1       -1     -1     -1   \n",
       "\n",
       "         pwtyp  hhparcel  \n",
       "1312         2     64747  \n",
       "1319         1     64747  \n",
       "1651         0     64747  \n",
       "1652         0    413627  \n",
       "1836         1    552382  \n",
       "...        ...       ...  \n",
       "4021852      0    538473  \n",
       "4024129      0    219352  \n",
       "4024130      0    219352  \n",
       "4025387      0    242547  \n",
       "4025388      0    242547  \n",
       "\n",
       "[699968 rows x 20 columns]"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_person[df_person['hhparcel'].isin(parcel_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'azone_lttrk_prop.csv' \n",
    "# This file specifies the light truck proportion of the vehicle fleet and is used in AssignVehicleType module.\n",
    "# This refers I believe to the main vehicle population; \n",
    "# Not changing the default for now, but should be looked up via MOVES or assumed flat?\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Geo'] = 'PSRC'\n",
    "df['Year'] = ['2018','2050']\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'azone_per_cap_inc.csv' \n",
    "# This file contains information on regional average per capita household and group quarters income in year 2010 dollars and is used in the PredictIncome module.\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Geo'] = 'PSRC'\n",
    "df['Year'] = ['2018','2050']\n",
    "\n",
    "# Get average income from h5 files\n",
    "syn_h5 = h5py.File(r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2018\\new_emp\\hh_and_persons.h5', 'r')\n",
    "df_hh = pd.DataFrame()\n",
    "for col in syn_h5['Household'].keys():\n",
    "    df_hh[col] = syn_h5['Household'][col][:]\n",
    "df_hh = df_hh[df_hh['hhparcel'].isin(parcel_list)]    \n",
    "df.loc[df['Year'] == '2018', ['HHIncomePC.2010','GQIncomePC.2010']] = df_hh['hhincome'].mean()\n",
    "\n",
    "syn_h5 = h5py.File(r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2050\\rtp_2050\\hh_and_persons.h5', 'r')\n",
    "df_hh = pd.DataFrame()\n",
    "for col in syn_h5['Household'].keys():\n",
    "    df_hh[col] = syn_h5['Household'][col][:]\n",
    "df_hh = df_hh[df_hh['hhparcel'].isin(parcel_list)] \n",
    "df.loc[df['Year'] == '2050', ['HHIncomePC.2010','GQIncomePC.2010']] = df_hh['hhincome'].mean()\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B Zones\n",
    "Block Group Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'bzone_transit_service.csv' \n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "# D4c\n",
    "# Aggregate frequency of transit service within 0.25 miles of block group boundary per hour during evening peak period \n",
    "# (Ref: EPA 2010 Smart Location Database) from GTFS.\n",
    "# See pg 23 https://www.epa.gov/sites/default/files/2021-06/documents/epa_sld_3.0_technicaldocumentationuserguide_may2021.pdf\n",
    "# Aggregate Frequency of Peak Hour Transit Service (D4c) \n",
    "\n",
    "# FIXME: do a comparison to our calculation to make sure it lines up with the EPA data for base year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_transit_service(run_dir, year):\n",
    "    # Transit stops do not have frequencies so we need to load the transit lines\n",
    "    df_transit = gpd.read_file(os.path.join(run_dir,r'inputs\\scenario\\networks\\shapefiles\\AM\\AM_edges.shp'))\n",
    "\n",
    "    df = pd.read_csv(os.path.join(run_dir,r'inputs\\scenario\\networks\\shapefiles\\AM\\AM_transit_segments.csv'))\n",
    "    df_headways = pd.read_csv(os.path.join(run_dir,r'inputs\\scenario\\networks\\headways.csv'))\n",
    "    df = df.merge(df_headways[['LineID','hdw_16to17']])\n",
    "\n",
    "    df_transit = df_transit.merge(df[['LineID','hdw_16to17','ij']], left_on='id', right_on='ij')\n",
    "\n",
    "    gdf_bg = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)\n",
    "    # Apply a quarter mile buffer around each block group\n",
    "    gdf_bg.geography = gdf_bg.buffer(5280/4.0)\n",
    "\n",
    "    gdf_joined = gpd.sjoin(gdf_bg, df_transit, how='left')\n",
    "\n",
    "    bg_headways = gdf_joined.groupby(['geoid20','LineID']).mean()[['hdw_16to17']].reset_index()\n",
    "\n",
    "    bg_headways['D4c'] = 60.0/bg_headways['hdw_16to17']\n",
    "    bg_headways['D4c'].replace(np.inf, 0, inplace=True)\n",
    "\n",
    "    bg_headways = bg_headways.groupby('geoid20').sum()[['D4c']]\n",
    "    bg_headways = bg_headways.reset_index()\n",
    "\n",
    "    # Make sure to include all block groups even if they have no transit\n",
    "    add_df = gdf_bg[-gdf_bg['geoid20'].isin(bg_headways['geoid20'])][['geoid20']]\n",
    "    add_df['D4c'] = 0\n",
    "    bg_headways = bg_headways.append(add_df)\n",
    "\n",
    "    bg_headways.rename(columns={'geoid20': 'Geo'}, inplace=True)\n",
    "    bg_headways['Year'] = year\n",
    "\n",
    "    bg_headways = bg_headways[bg_headways['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "    # geo_df = pd.read_csv(os.path.join(output_dir,r'..\\defs\\geo.csv'))\n",
    "    # bg_headways = bg_headways[bg_headways['Geo'].isin(geo_df['Bzone'].astype())]\n",
    "\n",
    "    return bg_headways\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\geopandas\\geodataframe.py:182: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  super(GeoDataFrame, self).__setattr__(attr, val)\n"
     ]
    }
   ],
   "source": [
    "run_dir = r'C:\\Workspace\\sc_2018_rtp_final\\soundcast'\n",
    "df = calculate_transit_service(run_dir, '2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\geopandas\\geodataframe.py:182: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  super(GeoDataFrame, self).__setattr__(attr, val)\n"
     ]
    }
   ],
   "source": [
    "run_dir = r'L:\\RTP_2022\\final_runs\\sc_rtp_2050_constrained_final\\soundcast'\n",
    "df_50 = calculate_transit_service(run_dir, '2050')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df_50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geo</th>\n",
       "      <th>D4c</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>530330028001</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>530330028002</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>530330028003</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>530330028004</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>530330028001</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>530330028002</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>530330028003</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>530330028004</td>\n",
       "      <td>47.0</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Geo   D4c  Year\n",
       "96  530330028001  25.0  2018\n",
       "97  530330028002  25.0  2018\n",
       "98  530330028003  25.0  2018\n",
       "99  530330028004  27.0  2018\n",
       "96  530330028001  42.0  2050\n",
       "97  530330028002  42.0  2050\n",
       "98  530330028003  42.0  2050\n",
       "99  530330028004  47.0  2050"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bzone_transit_service.csv'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_bg[gdf_bg['geoid20'] == '530330017024']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Process data at Block Group Level\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'bzone_carsvc_availability.csv'\n",
    "# This file contains the information about level of car service availability and is used in the AssignCarSvcAvailability module.\n",
    "\n",
    "#######################\n",
    "# FIXME: update with values based on density or some other data\n",
    "########################\n",
    "\n",
    "# Extract something about average weight times or population from existing model. \n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "_gdf_bg = gdf_bg[['geoid20']].copy()\n",
    "_gdf_bg.rename(columns={'geoid20': 'Geo'}, inplace=True)\n",
    "_gdf_bg_18 = _gdf_bg.copy()\n",
    "_gdf_bg_18['Year'] = '2018'\n",
    "_gdf_bg_18['CarSvcLevel'] = 'High'\n",
    "\n",
    "_gdf_bg_50 = _gdf_bg_18.copy()\n",
    "_gdf_bg_50['Year'] = '2050'\n",
    "\n",
    "df = _gdf_bg_18.append(_gdf_bg_50)\n",
    "\n",
    "# df = df[df['Geo'].isin(geo_df['Bzone'])]\n",
    "\n",
    "df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'bzone_dwelling_units.csv' \n",
    "# This file contains the number single-family, multi-family and group-quarter dwelling units and is used in the PredictHousing module.\n",
    "\n",
    "# Extract from parcel file; assuming 0 group quarter units \n",
    "df = parcel_18.groupby('geoid20').sum()[['SFUNITS','MFUNITS']].reset_index()\n",
    "df.rename(columns={'SFUNITS': 'SFDU', 'MFUNITS': 'MFDU', 'geoid20': 'Geo'}, inplace=True)\n",
    "df['GQDU'] = 0\n",
    "# df.iloc[0:6]['GQDU'] = 100     ################# FIXME: temp test\n",
    "df['Year'] = '2018'\n",
    "\n",
    "df_50 = parcel_18.groupby('geoid20').sum()[['SFUNITS','MFUNITS']].reset_index()\n",
    "df_50.rename(columns={'SFUNITS': 'SFDU', 'MFUNITS': 'MFDU', 'geoid20': 'Geo'}, inplace=True)\n",
    "df_50['GQDU'] = 0\n",
    "# df.iloc[0:6]['GQDU'] = 100     ################# FIXME: temp test\n",
    "df_50['Year'] = '2050'\n",
    "# SFDU, fmdu, GQDU\n",
    "\n",
    "df = df.append(df_50)\n",
    "\n",
    "df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'bzone_employment.csv' \n",
    "# This file contains the total, retail and service employment by zone and is used in the LocateEmployment module.\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "# 'Geo', 'Year', 'TotEmp', 'RetEmp', 'SvcEmp'\n",
    "df = parcel_18.groupby('geoid20').sum()[['EMPRET_P','EMPSVC_P','EMPTOT_P']].reset_index()\n",
    "df.rename(columns={'EMPRET_P': 'RetEmp', 'EMPSVC_P': 'SvcEmp', 'EMPTOT_P': 'TotEmp', 'geoid20': 'Geo'}, inplace=True)\n",
    "df['Year'] = '2018'\n",
    "\n",
    "df_50 = parcel_50.groupby('geoid20').sum()[['EMPRET_P','EMPSVC_P','EMPTOT_P']].reset_index()\n",
    "df_50.rename(columns={'EMPRET_P': 'RetEmp', 'EMPSVC_P': 'SvcEmp', 'EMPTOT_P': 'TotEmp', 'geoid20': 'Geo'}, inplace=True)\n",
    "df_50['Year'] = '2050'\n",
    "\n",
    "df = df.append(df_50)\n",
    "\n",
    "df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'bzone_hh_inc_qrtl_prop.csv'\n",
    "#  This file contains the proportion of households in 1st, 2nd, 3rd, and 4th quartile of household income and is used in the PredictHousing module.\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "def avg_income(year, run_dir):\n",
    "    # Get average income from h5 files\n",
    "    syn_h5 = h5py.File(os.path.join(run_dir, r'hh_and_persons.h5'), 'r')\n",
    "    df_hh = pd.DataFrame()\n",
    "    for col in syn_h5['Household'].keys():\n",
    "        df_hh[col] = syn_h5['Household'][col][:]\n",
    "    # df.loc[df['Year'] == '2018', ['HHIncomePC.2010','GQIncomePC.2010']] = df_hh['hhincome'].mean()\n",
    "\n",
    "\n",
    "    # Join the block group to the household\n",
    "    df_hh = df_hh.merge(parcel_18[['PARCELID','geoid20']], left_on='hhparcel', right_on='PARCELID')\n",
    "\n",
    "    _df = pd.DataFrame(pd.qcut(df_hh['hhincome'], 4, labels=['1','2','3','4']))\n",
    "    df_hh = df_hh.merge(_df, left_index=True, right_index=True)\n",
    "    _df = df_hh.pivot_table(index='geoid20', columns='hhincome_y', values='hhexpfac', aggfunc='sum')\n",
    "\n",
    "    df_sum = pd.DataFrame(_df[['1','2','3','4']].sum(axis=1)).reset_index()\n",
    "    df_sum.rename(columns={0: 'total_hh', 'geoid20': 'Geo'}, inplace=True)\n",
    "    _df = df_sum.merge(_df, left_on='Geo', right_index=True)\n",
    "    for i in [1,2,3,4]:\n",
    "        _df['HhPropIncQ'+str(i)] = _df[str(i)]/_df['total_hh']\n",
    "\n",
    "    # Make sure all block groups are available\n",
    "    # full_gdf_bg = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)\n",
    "    # missing_bg = full_gdf_bg[~full_gdf_bg['geoid20'].isin(_df['Geo'])]['geoid20']\n",
    "    # print(missing_bg)\n",
    "    # temp_df = _df.iloc[:len(missing_bg)].copy()\n",
    "    # # Set income distributions for empty block groups to uniform distribution\n",
    "    # temp_df[['HhPropIncQ1','HhPropIncQ2','HhPropIncQ3', 'HhPropIncQ4']] = 0.25\n",
    "    # temp_df['Geo'] = missing_bg['geoid20'].values\n",
    "    # _df = _df.append(temp_df)\n",
    "\n",
    "    # Filter for rows in the geo file\n",
    "    #### FIXME: This should depend on the final list of zones to be included\n",
    "    # geo_df = pd.read_csv(os.path.join(output_dir,r'..\\defs\\geo.csv'))\n",
    "    # _df = _df[_df['Geo'].isin(geo_df['Bzone'].astype('str'))]\n",
    "    \n",
    "    _df = _df[_df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "    _df['Year'] = year\n",
    "\n",
    "    return _df[df.columns]\n",
    "\n",
    "df = avg_income('2018', r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2018\\new_emp')\n",
    "df_50 = avg_income('2050', r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2050\\rtp_2050')\n",
    "df = df.append(df_50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_dir = r'R:\\e2projects_two\\SoundCast\\Inputs\\dev\\landuse\\2018\\new_emp'\n",
    "# syn_h5 = h5py.File(os.path.join(run_dir, r'hh_and_persons.h5'), 'r')\n",
    "# df_hh = pd.DataFrame()\n",
    "# for col in syn_h5['Household'].keys():\n",
    "#     df_hh[col] = syn_h5['Household'][col][:]\n",
    "# # df.loc[df['Year'] == '2018', ['HHIncomePC.2010','GQIncomePC.2010']] = df_hh['hhincome'].mean()\n",
    "\n",
    "\n",
    "# # Join the block group to the household\n",
    "# df_hh = df_hh.merge(parcel_18[['PARCELID','geoid20']], left_on='hhparcel', right_on='PARCELID')\n",
    "\n",
    "# _df = pd.DataFrame(pd.qcut(df_hh['hhincome'], 4, labels=['1','2','3','4']))\n",
    "# df_hh = df_hh.merge(_df, left_index=True, right_index=True)\n",
    "# _df = df_hh.pivot_table(index='geoid20', columns='hhincome_y', values='hhexpfac', aggfunc='sum')\n",
    "\n",
    "# df_sum = pd.DataFrame(_df[['1','2','3','4']].sum(axis=1)).reset_index()\n",
    "# df_sum.rename(columns={0: 'total_hh', 'geoid20': 'Geo'}, inplace=True)\n",
    "# _df = df_sum.merge(_df, left_on='Geo', right_index=True)\n",
    "# for i in [1,2,3,4]:\n",
    "#     _df['HhPropIncQ'+str(i)] = _df[str(i)]/_df['total_hh']\n",
    "\n",
    "# # Make sure all block groups are available\n",
    "# # full_gdf_bg = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)\n",
    "# # missing_bg = full_gdf_bg[~full_gdf_bg['geoid20'].isin(_df['Geo'])]['geoid20']\n",
    "# # print(missing_bg)\n",
    "# # temp_df = _df.iloc[:len(missing_bg)].copy()\n",
    "# # # Set income distributions for empty block groups to uniform distribution\n",
    "# # temp_df[['HhPropIncQ1','HhPropIncQ2','HhPropIncQ3', 'HhPropIncQ4']] = 0.25\n",
    "# # temp_df['Geo'] = missing_bg['geoid20'].values\n",
    "# # _df = _df.append(temp_df)\n",
    "\n",
    "# syn_h5.close()\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows in the geo file\n",
    "#### FIXME: This should depend on the final list of zones to be included\n",
    "# geo_df = pd.read_csv(os.path.join(output_dir,r'..\\defs\\geo.csv'))\n",
    "# _df = _df[_df['Geo'].isin(geo_df['Bzone']geo_df['Bzone'].astype('str'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _df[_df['Geo'].isin(geo_df['Bzone'].astype('str'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'bzone_lat_lon.csv' \n",
    "# This file contains the latitude and longitude of the centroid of the zone and is used in the LocateEmployment module.\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "# Get from intptlat and intpltlon\n",
    "###########################\n",
    "#### FIXME: ensure these are the correct values, not sure how they are calculated or their source\n",
    "###########################\n",
    "gdf_bg_18 = gdf_bg.copy()\n",
    "gdf_bg_18.rename(columns={'intptlat': 'Latitude', 'intptlon': 'Longitude', 'geoid20': 'Geo'}, inplace=True)\n",
    "gdf_bg_18['Year'] = '2018'\n",
    "\n",
    "gdf_bg_50 = gdf_bg_18.copy()\n",
    "gdf_bg_50['Year'] = '2050'\n",
    "\n",
    "df = gdf_bg_18.append(gdf_bg_50)[['Geo', 'Year', 'Latitude', 'Longitude']]\n",
    "\n",
    "df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'bzone_network_design.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "#  This file contains the intersection density in terms of pedestrian-oriented intersections having four or more legs per square mile and is used in the Calculate4DMeasures module.\n",
    "\n",
    "# d3bpo4 https://www.epa.gov/sites/default/files/2021-06/documents/epa_sld_3.0_technicaldocumentationuserguide_may2021.pdf\n",
    "# Intersection density in terms of pedestrian-oriented\n",
    "# intersections having four or more legs per square mile \n",
    "\n",
    "\n",
    "#######\n",
    "## Let's just use pandana lib for this\n",
    "def calculate_intersections(run_dir, year):\n",
    "    nodes = pd.read_csv(os.path.join(run_dir, r'inputs\\base_year\\all_streets_nodes.csv'), \n",
    "                    index_col='node_id')\n",
    "    links = pd.read_csv(os.path.join(run_dir, r'inputs\\base_year\\all_streets_links.csv'), \n",
    "                    index_col=None)\n",
    "\n",
    "    # get rid of circular links\n",
    "    links = links.loc[(links.from_node_id != links.to_node_id)]\n",
    "\n",
    "    # assign impedance\n",
    "    imp = pd.DataFrame(links.Shape_Length)\n",
    "    imp = imp.rename(columns = {'Shape_Length':'distance'})\n",
    "    links[['from_node_id','to_node_id']] = links[['from_node_id','to_node_id']].astype('int') \n",
    "\n",
    "    net = pdna.network.Network(nodes.x, nodes.y, links.from_node_id, links.to_node_id, imp)\n",
    "    all_nodes = pd.DataFrame(net.edges_df['from'].append(net.edges_df.to), columns = ['node_id'])\n",
    "\n",
    "    # get the frequency of each node, which is the number of intersecting ways\n",
    "    intersections_df = pd.DataFrame(all_nodes.node_id.value_counts())\n",
    "    intersections_df = intersections_df.rename(columns = {'node_id' : 'edge_count'})\n",
    "    intersections_df.reset_index(0, inplace = True)\n",
    "    intersections_df = intersections_df.rename(columns = {'index' : 'node_ids'})\n",
    "\n",
    "    # add a column for each way count\n",
    "    intersections_df['nodes4'] = np.where(intersections_df['edge_count']>3, 1, 0)\n",
    "\n",
    "    df = nodes.merge(intersections_df, left_index=True, right_on='node_ids')\n",
    "    # filter for all 4-way intersections\n",
    "    df = df[df['nodes4'] > 0]\n",
    "\n",
    "    # Convert to geopandas dataframe\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df, geometry=gpd.points_from_xy(df.x, df.y))\n",
    "    crs = {'init' : 'EPSG:2285'}\n",
    "    gdf.crs = crs\n",
    "\n",
    "    gdf = gpd.sjoin(gdf, gdf_bg)\n",
    "\n",
    "    df = gdf.groupby('geoid20').sum()[['nodes4']]\n",
    "\n",
    "    # Join with other block groups not including any 4-way intersections\n",
    "    df = gdf_bg.merge(df, on='geoid20', how='left')[['geoid20', 'nodes4']]\n",
    "    df['nodes4'].fillna(0, inplace=True)\n",
    "    df.rename(columns={'nodes4': 'D3bpo4', 'geoid20': 'Geo'}, inplace=True)\n",
    "    df['Year'] = year\n",
    "\n",
    "    # #### FIXME: This should depend on the final list of zones to be included\n",
    "    # geo_df = pd.read_csv(os.path.join(output_dir,r'..\\defs\\geo.csv'))\n",
    "    # df = df[df['Geo'].isin(geo_df['Bzone'])]\n",
    "\n",
    "    df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "df_18 = calculate_intersections(run_dir_18, '2018')\n",
    "df_50 = calculate_intersections(run_dir_50, '2050')\n",
    "df = df_18.append(df_50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'bzone_parking.csv' \n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df\n",
    "# This file contains the parking information and is used in the AssignParkingRestrictions module.\n",
    "\n",
    "# PkgSpacesPerSFDU: Average number of free parking spaces available to residents of single-family dwelling units\n",
    "# PkgSpacesPerMFDU: Average number of free parking spaces available to residents of multifamily dwelling units\n",
    "# PkgSpacesPerGQ: Average number of free parking spaces available to group quarters residents\n",
    "# PropWkrPay: Proportion of workers who pay for parking\n",
    "# PropCashOut: Proportions of workers paying for parking in a cash-out-buy-back program\n",
    "# PkgCost: Average daily cost for long-term parking (e.g. paid on monthly basis)\n",
    "\n",
    "### FIXME: see if we can get this info; shouldn't come from parcel data becasue this is limited to on-street parking\n",
    "\n",
    "# df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "# df\n",
    "# should mostly be available from parcels file\n",
    "\n",
    "# _df['avg_dy_price'] = _df['PARKDY_P']/_df['PPRICDYP']\n",
    "# _df['avg_hr_price'] = _df['PARKHR_P']/_df['PPRICHRP']\n",
    "\n",
    "def get_parking_restrictions(year):\n",
    "##### FIXME\n",
    "#############\n",
    "    _df = parcel_18.groupby('geoid20').sum()[['PARKDY_P','PARKHR_P','PPRICDYP','PPRICHRP','SFUNITS','MFUNITS']]\n",
    "    _df = _df.reset_index()\n",
    "    _df['PkgSpacesPerSFDU'] = 2    # Set equal to 2 for now\n",
    "    _df['PkgSpacesPerMFDU'] = 0.5    # Set equal to 0.5 for now\n",
    "    _df['PkgSpacesPerGQ'] = 0.5     # set to same as MF ?\n",
    "\n",
    "    df.columns\n",
    "    # \n",
    "    # PropWrkPay\n",
    "    # Is a variable in daysim but we do not populate it\n",
    "    _df['PropNonWrkTripPay'] = 0\n",
    "    _df['PropWkrPay'] = 0\n",
    "    _df['PropCashOut'] = 0\n",
    "    _df['PkgCost.2010'] = 0\n",
    "\n",
    "    _df.rename(columns={'geoid20': 'Geo'}, inplace=True)\n",
    "    _df['Year'] = year\n",
    "\n",
    "    return _df\n",
    "\n",
    "df18 = get_parking_restrictions('2018')[df.columns]\n",
    "df50 = get_parking_restrictions('2050')[df.columns]\n",
    "df = df18.append(df50)\n",
    "\n",
    "# df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'bzone_travel_demand_mgt.csv'\n",
    "_df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "# This file contains the information about workers and households participating in demand management programs and is used in the AssignDemandManagement module.\n",
    "\n",
    "def get_tdm(year):\n",
    "    _df = parcel_18.groupby('geoid20').count()[['PARCELID']].reset_index()\n",
    "    _df = _df.reset_index()\n",
    "    # could use output of transit pass ownership model or apply some off model thing based on survey data\n",
    "    # FIXME: update this from some other attribute\n",
    "\n",
    "    # EcoProp: Proportion of workers working in Bzone who participate in strong employee commute options program\n",
    "    # ImpProp: Proportion of households residing in Bzone who participate in strong individualized marketing program\n",
    "    _df['EcoProp'] = 0\n",
    "    _df['ImpProp'] = 0\n",
    "    _df.rename(columns={'geoid20': 'Geo'}, inplace=True)\n",
    "    _df['Year'] = year\n",
    "\n",
    "    return _df\n",
    "\n",
    "df18 = get_tdm('2018')[_df.columns]\n",
    "df50 = get_tdm('2050')[_df.columns]\n",
    "df = df18.append(df50)\n",
    "\n",
    "df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geo</th>\n",
       "      <th>Year</th>\n",
       "      <th>EcoProp</th>\n",
       "      <th>ImpProp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>530330028001</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>530330028002</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>530330028003</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>530330028004</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>530330028001</td>\n",
       "      <td>2050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>530330028002</td>\n",
       "      <td>2050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>530330028003</td>\n",
       "      <td>2050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>530330028004</td>\n",
       "      <td>2050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Geo  Year  EcoProp  ImpProp\n",
       "97   530330028001  2018        0        0\n",
       "98   530330028002  2018        0        0\n",
       "99   530330028003  2018        0        0\n",
       "100  530330028004  2018        0        0\n",
       "97   530330028001  2050        0        0\n",
       "98   530330028002  2050        0        0\n",
       "99   530330028003  2050        0        0\n",
       "100  530330028004  2050        0        0"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fname = 'bzone_unprotected_area.csv'\n",
    "# This file contains the information about unprotected (i.e., developable) area within the zone and is used in the Calculate4DMeasures module.\n",
    "_df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "# get developable area from UGA boundaries\n",
    "# ElmerGeo.DBO.tod_prcl_uga\n",
    "# gdf_shp = read_from_sde(connection_string, 'tod_prcl_uga', version, crs=crs, is_table=False)\n",
    "# gdf_shp = read_from_sde(connection_string, 'urban_growth_area', version, crs=crs, is_table=False)\n",
    "# gdf_shp = gdf_shp.dissolve()\n",
    "# gdf_shp['in_uga'] = 1\n",
    "\n",
    "def get_unprotected_area(year, _df):\n",
    "    # Use the block group shapefile with no water because we are calculating buildable area\n",
    "    gdf_bg = read_from_sde(connection_string, 'blockgrp2020_nowater', version, crs=crs, is_table=False)\n",
    "    gdf_bg['total_area'] = gdf_bg.area\n",
    "\n",
    "    # Add rural/town/urban definitions using regional geographies\n",
    "    rg_shp = read_from_sde(connection_string, 'regional_geographies_preferred_alternative', version, crs=crs, is_table=False)\n",
    "\n",
    "    # Load the UGA bounds\n",
    "    # Create these defintions based on regional geographies and UGA?\n",
    "    # UrbanArea: Area that is Urban and unprotected (i.e. developable) within the zone (Acres)\n",
    "    # TownArea: Area that is Town and unprotected within the zone (Acres)\n",
    "    # RuralArea: Area that is Rural and unprotected within the zone (Acres)\n",
    "    rg_shp['rg_propose_pa'].unique()\n",
    "    # rg_shp.columns\n",
    "    rg_dict = {'CitiesTowns': 'TownArea',\n",
    "                'Core': 'UrbanArea',\n",
    "                'UU': 'UrbanArea',\n",
    "                'Metro': 'UrbanArea',\n",
    "                'HCT': 'UrbanArea',\n",
    "                'Rural': 'RuralArea'}\n",
    "    rg_shp['urban_rural'] = rg_shp['rg_propose_pa'].map(rg_dict)\n",
    "\n",
    "    join_rg_gdf = gpd.overlay(gdf_bg, rg_shp, how='intersection')\n",
    "\n",
    "    join_rg_gdf['rg_area'] = join_rg_gdf.area\n",
    "\n",
    "    df = join_rg_gdf.pivot_table(index='geoid20', columns='urban_rural', aggfunc='sum', values='rg_area').fillna(0)\n",
    "    df = df.reset_index()\n",
    "    # df.drop('urban_rural', axis=1, inplace=True)\n",
    "\n",
    "    # Make sure all block groups are available\n",
    "    # Since we used the no_water version, there are some locations that were excluded\n",
    "    # full_gdf_bg = read_from_sde(connection_string, 'blockgrp2020', version, crs=crs, is_table=False)\n",
    "    # missing_bg = full_gdf_bg[~full_gdf_bg['geoid20'].isin(df['geoid20'])]\n",
    "    # print(missing_bg['geoid20'])\n",
    "    # temp_df = df.iloc[:len(missing_bg)].copy()\n",
    "\n",
    "    # temp_df[['UrbanArea','TownArea','RuralArea']] = 0\n",
    "    # temp_df['geoid20'] = missing_bg['geoid20'].values\n",
    "    # df = df.append(temp_df)\n",
    "\n",
    "    # Convert to acres\n",
    "    df[['RuralArea','TownArea','UrbanArea']] = df[['RuralArea','TownArea','UrbanArea']]/43560.0\n",
    "\n",
    "    df['Year'] = year\n",
    "    df.rename(columns={'geoid20': 'Geo'}, inplace=True)\n",
    "\n",
    "    return df[_df.columns]\n",
    "\n",
    "df_18 = get_unprotected_area('2018', _df)\n",
    "df_50 = get_unprotected_area('2050', _df)\n",
    "df = df_18.append(df_50)\n",
    "\n",
    "df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>urban_rural</th>\n",
       "      <th>Geo</th>\n",
       "      <th>Year</th>\n",
       "      <th>UrbanArea</th>\n",
       "      <th>TownArea</th>\n",
       "      <th>RuralArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>530330028001</td>\n",
       "      <td>2018</td>\n",
       "      <td>65.242410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>530330028002</td>\n",
       "      <td>2018</td>\n",
       "      <td>64.098424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>530330028003</td>\n",
       "      <td>2018</td>\n",
       "      <td>49.660364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>530330028004</td>\n",
       "      <td>2018</td>\n",
       "      <td>65.194315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>530330028001</td>\n",
       "      <td>2050</td>\n",
       "      <td>65.242410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>530330028002</td>\n",
       "      <td>2050</td>\n",
       "      <td>64.098424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>530330028003</td>\n",
       "      <td>2050</td>\n",
       "      <td>49.660364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>530330028004</td>\n",
       "      <td>2050</td>\n",
       "      <td>65.194315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "urban_rural           Geo  Year  UrbanArea  TownArea  RuralArea\n",
       "97           530330028001  2018  65.242410       0.0        0.0\n",
       "98           530330028002  2018  64.098424       0.0        0.0\n",
       "99           530330028003  2018  49.660364       0.0        0.0\n",
       "100          530330028004  2018  65.194315       0.0        0.0\n",
       "97           530330028001  2050  65.242410       0.0        0.0\n",
       "98           530330028002  2050  64.098424       0.0        0.0\n",
       "99           530330028003  2050  49.660364       0.0        0.0\n",
       "100          530330028004  2050  65.194315       0.0        0.0"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'bzone_urban-mixed-use_prop.csv'\n",
    "# This file contains the target proportion of households located in mixed-used neighborhoods in zone and is used in the CalculateUrbanMixMeasure module.\n",
    "\n",
    "_df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "# Not sure how to define this; from parcels aggregate to BG and consider any zone with employment and households as mixed use?\n",
    "\n",
    "\n",
    "def get_mixed_use(parcel_df, year, _df):\n",
    "    \n",
    "    # parcel_df = parcel_18.copy()\n",
    "    parcel_df['total_units'] = parcel_df['SFUNITS']+parcel_df['MFUNITS']\n",
    "    parcel_df['mixed_use'] = 0\n",
    "    parcel_df.loc[parcel_df['SFUNITS'] < parcel_df['total_units'], 'mixed_use'] = 1\n",
    "\n",
    "    # Calculate\n",
    "    parcel_df['wt_mixed_use'] = parcel_df['mixed_use']*parcel_df['total_units']\n",
    "\n",
    "    df = parcel_df.groupby('geoid20').sum()[['total_units','wt_mixed_use']].reset_index()\n",
    "    df['MixUseProp'] = df['wt_mixed_use']/df['total_units']\n",
    "    df['MixUseProp'].fillna(0, inplace=True)\n",
    "\n",
    "    df['Year'] = year\n",
    "    df.rename(columns={'geoid20': 'Geo'}, inplace=True)\n",
    "\n",
    "    return df[_df.columns]\n",
    "\n",
    "df_18 = get_mixed_use(parcel_18.copy(), '2018', _df)\n",
    "df_50 = get_mixed_use(parcel_50.copy(), '2050', _df)\n",
    "df = df_18.append(df_50)\n",
    "\n",
    "df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Modeller\\Anaconda3\\envs\\summary\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "fname = 'bzone_urban-town_du_proportions.csv'\n",
    "# This file contains proportion of Single-Family, Multi-Family and Group Quarter dwelling units within the urban portion of the zone and is used in the AssignLocTypes module.\n",
    "\n",
    "# from parcels file with land use types associated\n",
    "\n",
    "\n",
    "_df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "_df\n",
    "\n",
    "rg_shp = read_from_sde(connection_string, 'regional_geographies_preferred_alternative', version, crs=crs, is_table=False)\n",
    "\n",
    "\n",
    "def get_du_proportions(rg_shp, _parcel_df, year, _df):\n",
    "    # # parcel_18_gdf\n",
    "    gdf = gpd.sjoin(_parcel_df, rg_shp, how='left')\n",
    "\n",
    "    rg_dict = {'CitiesTowns': 'TownArea',\n",
    "                'Core': 'UrbanArea',\n",
    "                'UU': 'UrbanArea',\n",
    "                'Metro': 'UrbanArea',\n",
    "                'HCT': 'UrbanArea',\n",
    "                'Rural': 'RuralArea'}\n",
    "    gdf['urban_rural'] = gdf['rg_propose_pa'].map(rg_dict)\n",
    "\n",
    "    df = gdf.pivot_table(index='geoid20', columns='urban_rural', values=['MFUNITS','SFUNITS'], aggfunc='sum').fillna(0).reset_index()\n",
    "    \n",
    "    # for unit_type in ['SFUNITS','MFUNITS']:\n",
    "    #     df['PropTown'+unit_type[0:2]+'DU'] = (df[unit_type]['TownArea']/df[unit_type].sum(axis=1)).fillna(0)\n",
    "    #     df['PropUrban'+unit_type[0:2]+'DU'] = (df[unit_type]['UrbanArea']/df[unit_type].sum(axis=1)).fillna(0)\n",
    "\n",
    "    ######################\n",
    "    # FIXME: setting this match format from RVMPO, but not sure why it doesn't work as it\n",
    "    ######################\n",
    "    # df['PropUrbanSFDU'] = (df['SFUNITS']['UrbanArea']/df['SFUNITS'].sum(axis=1)).fillna(0)\n",
    "    df['PropUrbanSFDU'] = 1\n",
    "    df['PropUrbanMFDU'] = 1\n",
    "    df['PropTownSFDU'] = 0\n",
    "    df['PropTownMFDU'] = 0\n",
    "    df['PropTownGQDU'] = 0\n",
    "    df['PropUrbanGQDU'] = 0\n",
    "    df['Year'] = year\n",
    "    df.rename(columns={'geoid20': 'Geo'}, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    df.columns = df.columns.droplevel(1)\n",
    "\n",
    "    return df[_df.columns]\n",
    "\n",
    "df_18 = get_du_proportions(rg_shp, parcel_18_gdf, '2018', _df)\n",
    "df_50 = get_du_proportions(rg_shp, parcel_50_gdf, '2050', _df)\n",
    "df = df_18.append(df_50)\n",
    "\n",
    "df = df[df['Geo'].astype('str').isin(geoid_list)]\n",
    "\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geo</th>\n",
       "      <th>Year</th>\n",
       "      <th>PropUrbanSFDU</th>\n",
       "      <th>PropUrbanMFDU</th>\n",
       "      <th>PropUrbanGQDU</th>\n",
       "      <th>PropTownSFDU</th>\n",
       "      <th>PropTownMFDU</th>\n",
       "      <th>PropTownGQDU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>530330028001</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>530330028002</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>530330028003</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>530330028004</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>530330028001</td>\n",
       "      <td>2050</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>530330028002</td>\n",
       "      <td>2050</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>530330028003</td>\n",
       "      <td>2050</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>530330028004</td>\n",
       "      <td>2050</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Geo  Year  PropUrbanSFDU  PropUrbanMFDU  PropUrbanGQDU  \\\n",
       "97   530330028001  2018              1              1              0   \n",
       "98   530330028002  2018              1              1              0   \n",
       "99   530330028003  2018              1              1              0   \n",
       "100  530330028004  2018              1              1              0   \n",
       "97   530330028001  2050              1              1              0   \n",
       "98   530330028002  2050              1              1              0   \n",
       "99   530330028003  2050              1              1              0   \n",
       "100  530330028004  2050              1              1              0   \n",
       "\n",
       "     PropTownSFDU  PropTownMFDU  PropTownGQDU  \n",
       "97              0             0             0  \n",
       "98              0             0             0  \n",
       "99              0             0             0  \n",
       "100             0             0             0  \n",
       "97              0             0             0  \n",
       "98              0             0             0  \n",
       "99              0             0             0  \n",
       "100             0             0             0  "
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rg_shp = read_from_sde(connection_string, 'regional_geographies_preferred_alternative', version, crs=crs, is_table=False)\n",
    "# _parcel_df = parcel_18_gdf.copy()\n",
    "# gdf = gpd.sjoin(_parcel_df, rg_shp, how='left')\n",
    "\n",
    "# rg_dict = {'CitiesTowns': 'TownArea',\n",
    "#             'Core': 'UrbanArea',\n",
    "#             'UU': 'UrbanArea',\n",
    "#             'Metro': 'UrbanArea',\n",
    "#             'HCT': 'UrbanArea',\n",
    "#             'Rural': 'RuralArea'}\n",
    "# gdf['urban_rural'] = gdf['rg_propose_pa'].map(rg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = gdf.pivot_table(index='geoid20', columns='urban_rural', values=['MFUNITS','SFUNITS'], aggfunc='sum').fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# gdf = gpd.sjoin(_parcel_df, rg_shp, how='left')\n",
    "# _parcel_df = parcel_18_gdf.copy()\n",
    "# _parcel_df = _parcel_df[_parcel_df['PARCELID'] != 0] \n",
    "# gdf = gpd.sjoin(_parcel_df, rg_shp, how='left')\n",
    "# rg_dict = {'CitiesTowns': 'TownArea',\n",
    "#         'Core': 'UrbanArea',\n",
    "#         'UU': 'UrbanArea',\n",
    "#         'Metro': 'UrbanArea',\n",
    "#         'HCT': 'UrbanArea',\n",
    "#         'Rural': 'RuralArea'}\n",
    "# gdf['urban_rural'] = gdf['rg_propose_pa'].map(rg_dict)\n",
    "\n",
    "# df = gdf.pivot_table(index='geoid20', columns='urban_rural', values=['MFUNITS','SFUNITS'], aggfunc='sum').fillna(0).reset_index()\n",
    "\n",
    "# for lu_type in ['Rural','Town','Urban']:\n",
    "#     df['tot_'+lu_type] = df['MFUNITS'][lu_type+'Area']+df['SFUNITS'][lu_type+'Area']\n",
    "#     df['Prop'+lu_type+'SFDU'] = df['SFUNITS'][lu_type+'Area']/df['tot_'+lu_type]\n",
    "#     df['Prop'+lu_type+'MFDU'] = df['MFUNITS'][lu_type+'Area']/df['tot_'+lu_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite geo.csv to only include block groups with some dwelling units\n",
    "# missing_bg\n",
    "# missing_bg = [530330053041,530330228033,530619900020,530339901000,\n",
    "#             530350903001,530359901000,530530726035,530530729031,\n",
    "#             530530729072,530530729082,530610401004,530619900020,530619901000]\n",
    "# fname = 'geo.csv'\n",
    "# df = pd.read_csv(os.path.join(output_dir,'..\\defs',fname))\n",
    "# df = df[~df['Bzone'].isin(missing_bg)]\n",
    "# df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M Area\n",
    "Regional totals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Optional files, not sure if they need to be included with zeros or not\n",
    "\n",
    "######### FIXME: \n",
    "######### create empty files with na values based on existing\n",
    "#########\n",
    "######### do something like load the existing file, fill with our region name and model years, and add empty null values\n",
    "\n",
    "'marea_base_year_dvmt.csv'\n",
    "# This file is used to specify to adjust the dvmt growth factors and is optional (only needed if user wants to modify the values). The file is used in the Initialize (VETravelPerformance), CalculateBaseRoadDvmt and CalculateFutureRoadDvmt modules.\n",
    "\n",
    "'marea_congestion_charges.csv' \n",
    "# This file is used to specify the charges of vehicle travel for different congestion levels and is optional. The file is used in the Initialize (VETravelPerformance) and CalculateRoadPerformance modules.\n",
    "\n",
    "'marea_dvmt_split_by_road_class.csv' \n",
    "# This file is used to specify the dvmt split for different road classes and is optional. The file is used in the Initialize (VETravelPerformance) and CalculateBaseRoadDvmt modules.\n",
    "\n",
    "'marea_operations_deployment.csv' \n",
    "# This file is used to specify the proportion of dvmt affected by operations for different road classes and is optional. The file is used in the Initialize (VETravelPerformance) and CalculateRoadPerformance modules.\n",
    "\n",
    "'marea_transit_ave_fuel_carbon_intensity.csv' \n",
    "# This file is used to specify the average carbon intensity of fuel used by transit and is optional. The file is used in the Initialize (VETravelPerformance) module.\n",
    "\n",
    "'marea_transit_biofuel_mix.csv' \n",
    "#This file is used to specify the biofuel used by transit and is optional. The file is used in the Initialize (VETravelPerformance) and CalculateCarbonIntensity modules.\n",
    "\n",
    "'marea_transit_fuel.csv' \n",
    "# This file is used to specify the transit fuel proportions and is optional. The file is used in the Initialize (VETravelPerformance) and CalculateCarbonIntensity modules.\n",
    "\n",
    "'marea_transit_powertrain_prop.csv' \n",
    "# This file is used to specify the mixes of transit vehicle powertrains and is optional. The file is used in the Initialize (VETravelPerformance) and CalculatePtranEnergyAndEmissions modules.\n",
    "\n",
    "for fname in ['marea_transit_powertrain_prop.csv',\n",
    "            'marea_base_year_dvmt.csv',\n",
    "            'marea_congestion_charges.csv',\n",
    "            'marea_dvmt_split_by_road_class.csv',\n",
    "            'marea_operations_deployment.csv',\n",
    "            'marea_transit_ave_fuel_carbon_intensity.csv',\n",
    "            'marea_transit_biofuel_mix.csv',\n",
    "            'marea_transit_fuel.csv',\n",
    "            'marea_transit_powertrain_prop.csv']:\n",
    "    df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "    # print(fname)\n",
    "    df['Geo'] = regional_geo\n",
    "    if 'Year' in df.columns:\n",
    "        df['Year'] = df['Year'].replace({2010: 2018, 2038: 2050})\n",
    "    df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'marea_lane_miles.csv' \n",
    "# This file contains inputs on the numbers of freeway lane-miles and arterial lane-miles and is used in the AssignRoadMiles module.\n",
    "\n",
    "# FwyLaneMi: Lane-miles of roadways functionally classified as freeways or expressways in the urbanized portion of the metropolitan area\n",
    "# ArtLaneMi: Lane-miles of roadways functionally classified as arterials (but not freeways or expressways) in the urbanized portion of the metropolitan area\n",
    "\n",
    "def load_network_summary(filepath):\n",
    "    \"\"\"Load network-level results using a standard procedure. \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Congested network components by time of day\n",
    "    df.columns\n",
    "\n",
    "    # Get freeflow from 20to5 period\n",
    "\n",
    "    # Exclude trips taken on non-designated facilities (facility_type == 0)\n",
    "    # These are artificial (weave lanes to connect HOV) or for non-auto uses \n",
    "    df = df[df['data3'] != 0]    # data3 represents facility_type\n",
    "\n",
    "    # calculate total link VMT and VHT\n",
    "    df['VMT'] = df['@tveh']*df['length']\n",
    "    df['VHT'] = df['@tveh']*df['auto_time']/60\n",
    "\n",
    "    # Define facility type\n",
    "    df.loc[df['data3'].isin([1,2]), 'facility_type'] = 'highway'\n",
    "    df.loc[df['data3'].isin([3,4,6]), 'facility_type'] = 'arterial'\n",
    "    df.loc[df['data3'].isin([5]), 'facility_type'] = 'connector'\n",
    "\n",
    "    # Calculate delay\n",
    "    # Select links from overnight time of day\n",
    "    delay_df = df.loc[df['tod'] == '20to5'][['ij','auto_time']]\n",
    "    delay_df.rename(columns={'auto_time':'freeflow_time'}, inplace=True)\n",
    "\n",
    "    # Merge delay field back onto network link df\n",
    "    df = pd.merge(df, delay_df, on='ij', how='left')\n",
    "\n",
    "    # Calcualte hourly delay\n",
    "    df['total_delay'] = ((df['auto_time']-df['freeflow_time'])*df['@tveh'])/60    # sum of (volume)*(travtime diff from freeflow)\n",
    "\n",
    "    df['county'] =df['@countyid'].map({33: 'King',\n",
    "                                      35: 'Kitsap',\n",
    "                                      53: 'Pierce',\n",
    "                                      61: 'Snohomish'})\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_lane_miles(year, run_dir):\n",
    "    df_network = load_network_summary(os.path.join(run_dir,r'outputs\\network','network_results.csv'))\n",
    "    # Select mid-day network\n",
    "    gdf = df_network[df_network['tod'] == '10to14']\n",
    "    gdf['Lane Miles'] = gdf['length']*gdf['num_lanes']\n",
    "\n",
    "    ul3_dict = {\n",
    "        0: 'Rail/Walk/Ferry',\n",
    "        1: 'Freeway',\n",
    "        2: 'Expressway',\n",
    "        3: 'Urban Arterial',\n",
    "        4: 'One-way Arterial',\n",
    "        5: 'Centroid Connector',\n",
    "        6: 'Rural Arterial'\n",
    "    }\n",
    "\n",
    "    gdf['Facility Group'] = gdf['data3'].map(ul3_dict)\n",
    "    df = gdf.groupby(['Facility Group','data3']).sum()[['Lane Miles']].sort_values('data3').reset_index()\n",
    "\n",
    "    _df = pd.DataFrame([df[df['Facility Group'].isin(['Freeway','Expressway'])]['Lane Miles'].sum(),\n",
    "                    df[df['Facility Group'].isin(['Urban Arterial','Rural Arterial', 'One-way arterial'])]['Lane Miles'].sum()]).T\n",
    "    _df.columns = ['FwyLaneMi', 'ArtLaneMi']\n",
    "    _df['Year'] = year\n",
    "    \n",
    "    _df['Geo'] = regional_geo\n",
    "    \n",
    "    return _df\n",
    "\n",
    "regional_geo = 'PSRC'\n",
    "df18 = get_lane_miles('2018', r'L:\\RTP_2022\\final_runs\\sc_2018_rtp_final\\soundcast')\n",
    "df50 = get_lane_miles('2050', r'L:\\RTP_2022\\final_runs\\sc_rtp_2050_constrained_final\\soundcast')\n",
    "\n",
    "df = df18.append(df50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'marea_speed_smooth_ecodrive.csv'\n",
    "# This input file supplies information of deployment of speed smoothing and ecodriving by road class and vehicle type and is used in the CalculateMpgMpkwhAdjustments module.\n",
    "\n",
    "# Set to zero for now\n",
    "# FwySmooth:Fractional deployment of speed smoothing traffic management on freeways, where 0 is no deployment and 1 is the full potential fuel savings\n",
    "# ArtSmooth: Fractional deployment of speed smoothing traffic management on arterials, where 0 is no deployment and 1 is the full potential fuel savings\n",
    "# LdvEcoDrive: Eco-driving penetration for light-duty vehicles; the fraction of vehicles from 0 to 1\n",
    "# HvyTrkEcoDrive: Eco-driving penetration for heavy-duty vehicles; the fraction of vehicles from 0 to 1\n",
    "\n",
    "df = pd.DataFrame([0,0,0,0]).T\n",
    "df.columns = ['FwySmooth','ArtSmooth','LdvEcoDrive','HvyTrkEcoDrive']\n",
    "df['Year'] = '2018'\n",
    "df['Geo'] = regional_geo\n",
    "\n",
    "\n",
    "df50 = pd.DataFrame([0,0,0,0]).T\n",
    "df50.columns = ['FwySmooth','ArtSmooth','LdvEcoDrive','HvyTrkEcoDrive']\n",
    "df50['Year'] = '2050'\n",
    "df50['Geo'] = regional_geo\n",
    "\n",
    "df = df.append(df50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'marea_transit_service.csv' \n",
    "# This file contains annual revenue-miles for different transit modes for metropolitan area and is used in the AssignTransitService module.\n",
    "\n",
    "# Get from Stefan's transit tool\n",
    "import transit_service_analyst\n",
    "import transit_service_analyst as tsa\n",
    "\n",
    "path = r'R:/e2projects_two/Angela/transit_routes_2018/latest_combined'\n",
    "transit_analyst = tsa.load_gtfs(path, '20180417', 0, 1680)\n",
    "\n",
    "geo_df = transit_analyst.get_lines_gdf()\n",
    "\n",
    "# DRRevMi: Annual revenue-miles of demand-responsive public transit service\n",
    "# VPRevMi: Annual revenue-miles of van-pool and similar public transit service\n",
    "# MBRevMi: Annual revenue-miles of standard bus public transit service\n",
    "# RBRevMi: Annual revenue-miles of rapid-bus and commuter bus public transit service\n",
    "# MGRevMi: Annual revenue-miles of monorail and automated guideway public transit service\n",
    "# SRRevMi: Annual revenue-miles of streetcar and trolleybus public transit service\n",
    "# HRRevMi: Annual revenue-miles of light rail and heavy rail public transit service\n",
    "# CRRevMi: Annual revenue-miles of commuter rail, hybrid rail, cable car, and aerial tramway public transit service\n",
    "\n",
    "# GTFS Routes Types:\n",
    "# 0 - Tram, Streetcar, Light rail. Any light rail or street level system within a metropolitan area.\n",
    "# 1 - Subway, Metro. Any underground rail system within a metropolitan area.\n",
    "# 2 - Rail. Used for intercity or long-distance travel.\n",
    "# 3 - Bus. Used for short- and long-distance bus routes.\n",
    "# 4 - Ferry. Used for short- and long-distance boat service.\n",
    "# 5 - Cable tram. Used for street-level rail cars where the cable runs beneath the vehicle, e.g., cable car in San Francisco.\n",
    "# 6 - Aerial lift, suspended cable car (e.g., gondola lift, aerial tramway). Cable transport where cabins, cars, gondolas or open chairs are suspended by means of one or more cables.\n",
    "# 7 - Funicular. Any rail system designed for steep inclines.\n",
    "# 11 - Trolleybus. Electric buses that draw power from overhead wires using poles.\n",
    "# 12 - Monorail. Railway in which the track consists of a single rail or a beam.\n",
    "\n",
    "#########################\n",
    "# FIXME! how to code ferry? coding as catch-all commuter/hybrid rail plus cable car and aerial tramway\n",
    "\n",
    "# FIXME! get a GTFS for future year or pull form network?\n",
    "\n",
    "#########################\n",
    "\n",
    "route_type_map = {\n",
    "    0: 'HRRevMi',   # streetcar/light rail -> light/heavy rail\n",
    "    1: 'HRRevMi',   # subway/metro -> light/heavy rail\n",
    "    2: 'CRRevMi',   # commuter rail plus cable car, aerial tram\n",
    "    3: 'MBRevMi',   # bus -> standard bus\n",
    "    4: 'CRRevMi',  # ferry -> commuter rail plus cable car, aerial tram\n",
    "    5: 'CRRevMi',   # cable tram ->  ' '\n",
    "    6: 'CRRevMi',   # aerial lift -> ' '\n",
    "    7: 'CRRevMi',   # funicular -> ' '\n",
    "    11: 'SRRevMi',  # trolleybus -> streetcar and trolleybus service\n",
    "    12: 'MGRevMi',  # monorail\n",
    "}\n",
    "\n",
    "def get_transit_miles(geo_df, fname, year):\n",
    "    geo_df['new_route_type'] = geo_df['route_type'].map(route_type_map)\n",
    "    _df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "\n",
    "    geo_df = geo_df.to_crs(crs)\n",
    "    geo_df['miles'] = geo_df.length/5280.0\n",
    "    df = geo_df.groupby('new_route_type').sum()[['miles']].T\n",
    "    for col in ['DRRevMi','VPRevMi','MBRevMi','RBRevMi','MGRevMi','SRRevMi','HRRevMi','CRRevMi']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "    df['Year'] = year\n",
    "    df['Geo'] = regional_geo\n",
    "\n",
    "    return df\n",
    "\n",
    "df_18 = get_transit_miles(geo_df, fname, '2018')\n",
    "df_50 = get_transit_miles(geo_df, fname, '2050')\n",
    "df = df_18.append(df_50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Optional ##########\n",
    "\n",
    "'other_ops_effectiveness.csv'\n",
    "# This file is used to specify the delay effects of operations in different road classes and is optional (only needed if user wants to modify the values). The file is used in the Initialize (VETravelPerformance) and CalculateRoadPerformance modules.\n",
    "\n",
    "'region_ave_fuel_carbon_intensity.csv'\n",
    "#This file is used to specify the average carbon density for different vehicle types and is optional (only needed if user wants to modify the values). The file is used in the Initialize (VETravelPerformance) and CalculateCarbonIntensity modules.\n",
    "\n",
    "'region_base_year_hvytrk_dvmt.csv'\n",
    "# This file is used to specify the heavy truck dvmt for base year and is optional. The file is used in the Initialize (VETravelPerformance), CalculateBaseRoadDvmt and CalculateFutureRoadDvmt modules.\n",
    "\n",
    "'region_carsvc_powertrain_prop.csv'\n",
    "# This file is used to specify the powertrain proportion of car services and is optional. The file is used in the Initialize (VETravelPerformance), AssignHhVehiclePowertrain and AdjustHhVehicleMpgMpkwh modules.\n",
    "\n",
    "'region_comsvc_powertrain_prop.csv'\n",
    "#This file is used to specify the powertrain proportion of commercial vehicles and is optional. The file is used in the Initialize (VEPowertrainsAndFuels) and CalculateComEnergyAndEmissions modules.\n",
    "\n",
    "'region_hvytrk_powertrain_prop.csv'\n",
    "#This file is used to specify the powertrain proportion of heavy duty trucks and is optional. The file is used in the Initialize (VEPowertrainsAndFuels) and CalculateComEnergyAndEmissions modules.\n",
    "\n",
    "for fname in ['other_ops_effectiveness.csv',\n",
    "            'region_ave_fuel_carbon_intensity.csv',\n",
    "            'region_base_year_hvytrk_dvmt.csv',\n",
    "            'region_carsvc_powertrain_prop.csv',\n",
    "            'region_comsvc_powertrain_prop.csv',\n",
    "            'region_hvytrk_powertrain_prop.csv']:\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "    except:\n",
    "        print(fname + ': file not available')\n",
    "        next\n",
    "        # print(fname)\n",
    "    df['Geo'] = regional_geo\n",
    "    if 'Year' in df.columns:\n",
    "        df['Year'] = df['Year'].replace({2010: 2018, 2038: 2050})\n",
    "    df.to_csv(os.path.join(output_dir,fname), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following regional levels inputs are required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_comsvc_lttrk_prop.csv'\n",
    "# This file supplies data for the light truck proportion of commercial vehicles and is used in the CalculateComEnergyAndEmissions module.\n",
    "\n",
    "########## FIXME: assuming 50% for now?\n",
    "\n",
    "\n",
    "# ComSvcLtTrkProp: Regional proportion of commercial service vehicles that are light trucks Here is a snapshot of the file:\n",
    "\n",
    "df = pd.DataFrame([0.5]).T\n",
    "df.columns = ['ComSvcLtTrkProp']\n",
    "df['Year'] = '2018'\n",
    "df['Geo'] = regional_geo\n",
    "\n",
    "\n",
    "df50 = pd.DataFrame([0.5]).T\n",
    "df50.columns = ['ComSvcLtTrkProp']\n",
    "df50['Year'] = '2050'\n",
    "df50['Geo'] = regional_geo\n",
    "\n",
    "df = df.append(df50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_hh_driver_adjust_prop.csv'\n",
    "#This file specifies the relative driver licensing rate relative to the model estimation data year and is used in the AssignDrivers module.\n",
    "\n",
    "## FIXME: set to 1\n",
    "\n",
    "# Drv15to19AdjProp: Target proportion of unadjusted model number of drivers 15 to 19 years old (1 = no adjustment)\n",
    "# Drv20to29AdjProp: Target proportion of unadjusted model number of drivers 20 to 29 years old (1 = no adjustment)\n",
    "# Drv30to54AdjProp: Target proportion of unadjusted model number of drivers 30 to 54 years old (1 = no adjustment)\n",
    "# Drv55to64AdjProp: Target proportion of unadjusted model number of drivers 55 to 64 years old (1 = no adjustment)\n",
    "# Drv65PlusAdjProp: Target proportion of unadjusted model number of drivers 65 or older (1 = no adjustment)\n",
    "\n",
    "\n",
    "df = pd.DataFrame([1,1,1,1,1]).T\n",
    "df.columns = ['Drv15to19AdjProp','Drv20to29AdjProp','Drv30to54AdjProp','Drv55to64AdjProp','Drv65PlusAdjProp']\n",
    "df['Year'] = '2018'\n",
    "df['Geo'] = regional_geo\n",
    "# df\n",
    "\n",
    "df50 = pd.DataFrame([1,1,1,1,1]).T\n",
    "df50.columns = ['Drv15to19AdjProp','Drv20to29AdjProp','Drv30to54AdjProp','Drv55to64AdjProp','Drv65PlusAdjProp']\n",
    "df50['Year'] = '2050'\n",
    "df50['Geo'] = regional_geo\n",
    "df_50\n",
    "df = df.append(df50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fname = 'region_prop_externalities_paid.csv'\n",
    "#This file supplies data for climate change and other social costs and is used in the CalculateVehicleOperatingCost module.\n",
    "\n",
    "# PropClimateCostPaid: Proportion of climate change costs paid by users (i.e. ratio of carbon taxes to climate change costs\n",
    "# PropOtherExtCostPaid: Proportion of other social costs paid by users\n",
    "\n",
    "df = pd.DataFrame([0,0]).T\n",
    "df.columns = ['PropClimateCostPaid','PropOtherExtCostPaid']\n",
    "df['Year'] = '2018'\n",
    "df['Geo'] = regional_geo\n",
    "# df\n",
    "\n",
    "df50 = pd.DataFrame([0,0]).T\n",
    "df50.columns = ['PropClimateCostPaid','PropOtherExtCostPaid']\n",
    "df50['Year'] = '2050'\n",
    "df50['Geo'] = regional_geo\n",
    "df_50\n",
    "df = df.append(df50)\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import filecmp\n",
    "import glob\n",
    "# print()\n",
    "files_target = glob.glob(r\"C:\\Workspace\\VisionEval\\models\\VERSPM\\inputs\\*\")\n",
    "files_new = glob.glob(r\"C:\\Workspace\\VisionEval\\input_creation\\psrc_inputs\\*\")\n",
    "# match, mismatch, errors = filecmp.cmpfiles('C:\\Workspace\\VisionEval\\input_creation\\psrc_inputs', 'C:\\Workspace\\VisionEval\\models\\VERSPM\\inputs', files)\n",
    "\n",
    "\n",
    "# Get rap file names only\n",
    "files_target = [i.split('\\\\')[-1] for i in files_target]\n",
    "files_new = [i.split('\\\\')[-1] for i in files_new]\n",
    "\n",
    "\n",
    "[i for i in files_target if i not in files_new]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the remaining missing files\n",
    "# Keeping default values from RVMPO for now\n",
    "\n",
    "fname = 'azone_hh_ave_veh_per_driver.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'azone_hh_lttrk_prop.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_base_year_dvmt.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "# df['Year'] = ['2018','2050']\n",
    "# df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_co2e_costs.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "# df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_comsvc_ave_veh_age.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "# df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_comsvc_veh_mean_age.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "# df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_hh_ave_driver_per_capita.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "# df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'region_road_cost.csv'\n",
    "df = pd.read_csv(os.path.join(input_dir,fname))\n",
    "df['Year'] = ['2018','2050']\n",
    "# df['Geo'] = regional_geo\n",
    "df.to_csv(os.path.join(output_dir,fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a01578ef7fc98460838ddfd60bd3288ea700197594b0894c527f4f3349251842"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('summary': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
